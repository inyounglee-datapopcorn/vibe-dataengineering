# 📝 2026-01-13

## 1교시 09:00 - 10:00

#### 📌 강의 핵심 요약
- 데이터베이스(Supabase)에 데이터를 적재하는 다양한 방식(API/ORM vs SQL)과 그 효율성을 비교합니다.
- 데이터 처리의 두 가지 큰 흐름인 **배치(Batch)**와 **스트리밍(Streaming)**의 개념 및 실무 적용 기준을 학습합니다.
- Airflow와 Flask에서 동적 시스템 활용을 위한 **Jinja2 템플릿**의 기초 문법과 활용법을 익힙니다.

#### 📚 상세 내용 정리

##### 1. 데이터 적재 방식: API(ORM) vs SQL
- **SQL 방식 (DML/DDL)**: `CREATE TABLE`, `INSERT INTO` 등의 명령어를 직접 수행. 소량의 데이터나 테이블 구조 정의(DDL) 시 유용하지만, 대량의 행(Row)을 하나씩 `INSERT`하는 것은 비효율적입니다.
- **API/ORM 방식**: 객체(Object)를 데이터베이스에 직접 매핑하여 적재. 복잡한 SQL 쿼리 없이 구조화된 데이터(CSV 등)를 통째로 꽂아 넣을 수 있어 대량 적재 시 생산성과 속도가 높습니다.

##### 2. 데이터 처리 방식: 배치 vs 스트리밍
- **배치(Batch) 처리**: 데이터를 일정량씩 묶어서 한 번에 처리하는 방식. (예: 100만 명에게 재난 문자 발송 시 10만 명씩 끊어서 발송) 리소스 관점에서 훨씬 경제적이며 대부분의 업무에 적합합니다.
- **스트리밍(Streaming) 처리**: 데이터가 발생하는 즉시 실시간으로 흐르듯 처리하는 방식. (예: 주식, 코인 시세) 실시간성이 극도로 중요한 경우에만 사용하며, 구현 리소스가 많이 듭니다.

##### 3. Jinja2 템플릿 엔진
- **개념**: 파이썬 환경(Flask, Airflow 등)에서 데이터나 함수를 동적으로 불러오기 위해 사용되는 템플릿 엔진입니다.
- **기본 문법**:
  - **변수 호출**: `{{ variable_name }}` (예: `{{ ds }}`)
  - **제어 구조**: `{% if ... %}`, `{% for ... %}`, `{% endfor %}` (반복문/조건문 사용 시 `%` 기호 활용)
- **Airflow 활용**: 실행 날짜(`ds`), 타임스탬프(`ts`), 변수(`var.value`) 등을 런타임에 동적으로 삽입할 때 사용합니다.

##### 4. [Q&A] 데이터 엔지니어링 설계 원칙
- **질문**: 계산 연산을 BI 도구에서 하는 것이 좋은가, 앞단(SQL/DB)에서 미리 해두는 것이 좋은가?
- **답변**: 무조건 **앞단에서 사전 계산(Pre-calculation)**해 두는 것이 원칙입니다. BI 도구(Tableau 등)는 시각화(Showing)에 최적화되어야 하며, 연산 부하를 줄여야 대시보드 로딩 속도(1~2초 내외)를 확보할 수 있습니다.

#### 💡 핵심 포인트
- **데이터 엔지니어의 역할**: 사용자가 지연 없이 데이터를 볼 수 있도록 모든 연산과 전처리를 미리 완료해 두는 것입니다.
- **성능 최적화**: 대량 데이터 처리 시에는 `INSERT INTO` 대신 배치성 API 호출을 고려해야 합니다.

#### ✅ 실습/과제
- (2교시 예정) Slack API 연동 및 SMTP 메일 전송 환경 설정 실습.

#### 🔗 관련 키워드
- Supabase API, ORM, Batch vs Streaming, Jinja2 Template, Pre-calculation, Airflow Macros.

## 2교시 10:00 - 11:00

#### 📌 강의 핵심 요약
- GitHub 리포지토리의 보안 설정 변경 방법과 코드 공유 시의 저작권 및 보안 가이드를 학습합니다.
- Slack Incoming Webhook을 활용한 메시지 전송 원리와 설정 단계를 상세히 파헤칩니다.
- 실제 기업형 워크스페이스에서 마주할 수 있는 앱 설치 권한 이슈와 그에 따른 대안(SMTP)을 검토합니다.

#### 📚 상세 내용 정리

##### 1. GitHub 보안 및 가시성 관리 상세
- **가시성 변경 프로젝트**: `Settings` 탭 → 최하단 `Danger Zone` → `Change visibility` 클릭 → `Make private` 선택.
- **프라이빗 전환 사유**: 
  - 리포지토리 내의 코드는 강사의 커스텀 로직과 외부 자료를 참고하여 작성되었으므로 전체 공개 시 저작권 이슈가 발생할 수 있습니다.
  - 협업자(Collaborator)로 등록된 수강생들은 프라이빗 상태에서도 여전히 접근이 가능합니다.
- **코드 공유 원칙**: 블로그 등에 기록할 때는 전체 소스 코드를 그대로 올리기보다, 구현 결과물과 핵심 로직의 "포인트" 위주로 작성하는 것을 지양/권장합니다.

##### 2. Slack Webhook 연동 및 설정 프로세스
- **설정 사이트**: [api.slack.com/apps](https://api.slack.com/apps) 접속.
- **주요 설정 메뉴**:
  - **Incoming Webhooks**: 외부 서비스(Airflow 등)에서 슬랙으로 데이터를 쏠 수 있는 종단점(Endpoint)을 활성화합니다. 활성화 시 `Webhook URL`이 생성됩니다.
  - **OAuth & Permissions**: 앱이 수행할 수 있는 작업의 범위(`Scopes`)를 정의합니다. (예: `chat:write`, `incoming-webhook` 등)
- **Airflow 연결**: `SlackWebhookOperator`를 사용하며, 서버의 Connection 설정에서 `conn_id`를 지정하고 Webhook URL의 키값을 등록합니다.

##### 3. 실습 중 발생한 권한 이슈 (Troubleshooting)
- **문제 발생**: `Incoming Webhooks` 활성화 시 '관리자 승인 필요' 또는 권한 부족으로 인해 설치가 차단되는 현상이 발생했습니다.
- **현장 진단**: 강사와 수강생 모두 워크스페이스의 'Owner' 또는 'Admin' 권한이 아닌 'Instructor' 권한만 있어 앱 배포가 제한됨을 확인했습니다.
- **해결 방안**: 운영진(병희 님 등)에게 권한 개방을 요청하거나, 보안 정책상 불가할 경우 개인 워크스페이스에서 테스트한 후 본 수업은 **SMTP(이메일)** 알림 실습으로 비중을 옮기기로 결정했습니다.

#### 💡 핵심 포인트
- **보안의 생활화**: 내가 의도적으로 공개할 데이터가 아니라면 기본적으로 프라이빗 설정을 유지하는 것이 좋습니다.
- **환경의 제약 이해**: 실제 실무 환경에서는 보안 정책으로 인해 특정 API나 웹훅 사용이 제약될 수 있으며, 이때 차선책(Email, Webhook Proxy 등)을 즉각 떠올릴 수 있어야 합니다.

#### ✅ 실습/과제
- 개인 슬랙 워크스페이스를 생성하여 `Incoming Webhooks` URL 발급받기.
- Airflow 커넥션에 해당 URL을 등록하여 `SlackWebhookOperator` 테스트 코드 실행.

#### 🔗 관련 키워드
- Danger Zone, Private Repository, Incoming Webhooks, OAuth Scopes, Admin Permission, Airflow Connections.

---

## 3교시 11:00 - 12:00

#### 📌 강의 핵심 요약
- GitHub Actions를 통한 CI/CD 배포 과정에서의 복합적인 오류 상황(권한, 설정 고정)을 디버깅합니다.
- 도커 컴포즈(Docker Compose) 환경에서의 환경 변수 우선순위와 설정 고기능성을 이해합니다.
- 문제 해결을 위한 개발자의 마인드셋(변경 이력 기억, 빠른 가설 검증)을 학습합니다.

#### 📚 상세 내용 정리

##### 1. 배포 자동화와 CI/CD 트러블슈팅
- **반복적인 배포 테스트**: 배포 자동화를 구축한 이후 약 100회 이상의 배포(`Actions` 실행)가 이루어졌으며, 기술적 성숙도가 높아짐에 따라 수 주의 작업을 하루 만에 처리할 수 있음을 확인했습니다.
- **환경 변수 충돌**: Airflow UI에서 커넥션 정보를 수정해도 반영되지 않던 이유는, 해당 정보가 `docker-compose.yaml` 파일 내에 **하드코딩(고정)**되어 있었기 때문입니다. 컨테이너 환경에서는 파일에 명시된 환경 변수가 UI 설정보다 우선순위가 높을 수 있음을 인지해야 합니다.

##### 2. 리포지토리 가시성과 배포 권한 문제
- **상황**: 2교시 때 보안을 위해 리포지토리를 **Private**으로 바꾼 직후 배포 자동화가 중단되었습니다.
- **원인**: 배포 서버나 GitHub Actions 내의 `checkout` 과정에서 프라이빗 리포지토리에 접근하기 위한 인증 정보(SSH Key, PAT 등)가 추가로 설정되지 않았기 때문입니다.
- **조치**: 실습 시간 지연을 방지하기 위해 일시적으로 리포지토리를 다시 **Public**으로 전환하여 배포 루프를 정상화했습니다.

##### 3. 효율적인 디버깅을 위한 가이드
- **변경 이력 추적**: 에러가 났을 때 "내가 무엇을 바꿨는가?"를 기억하는 것이 90%입니다. 하기 전과 한 후의 차이점(Diff)을 명확히 인지해야 합니다.
- **커밋 메시지의 중요성**: 변경 내용 없이 커밋하거나 의미 없는 메시지를 남기는 습관을 지양해야 합니다. `Add: Slack logic`, `Fix: Permission denied` 등 명확한 기록이 추후 디버깅 속도를 결정합니다.
- **실수 대처**: 실수는 누구나 하지만, 원인을 빠르게 파악(`프라이빗 전환 때문인가?`, `오타인가?`)하고 실행에 옮기는 것이 실무자의 역량입니다.

##### 4. 오후 세션 준비
- **Slack 연동 재검토**: 단순 API 호출 방식보다는 웹훅 방식이 안정적이므로, 점심 직후 웹훅을 통한 알림 성공 사례를 공유하고 다음 단계로 넘어갈 예정입니다.

#### 💡 핵심 포인트
- **디버깅 1원칙**: "가장 최근에 바꾼 것이 범인이다."
- **자동화의 명암**: 자동화는 편리하지만, 보안 설정(Private 전환 등) 하나로 전체 파이프라인이 멈출 수 있으므로 연쇄적인 영향을 항상 고려해야 합니다.

#### ✅ 실습/과제
- GitHub Actions 탭에서 배포 로그를 확인하고, `Success` 상태와 실제 Airflow DAG 반영 여부 크로스 체크.
- 점심시간 이후 13:00부터 SMTP 및 Slack 실전 메시지 전송 실습 재개.

#### 🔗 관련 키워드
- CI/CD Debugging, Environment Variable Priority, Repository Visibility, Commit Message Convention, GitHub Actions Execution.


## 🍽️ 점심시간 12:00 - 13:00

## 4교시 13:00 - 14:00

#### 📌 강의 핵심 요약
- Airflow와 Supabase(PostgreSQL) 연동 과정에서 발생하는 기술적 장애를 분석하고 해결합니다.
- 시니어 엔지니어가 로그와 AI를 활용해 문제를 추적하고 해결하는 실무적인 디버깅 프로세스를 체득합니다.

#### 📚 상세 내용 정리

##### 1. Supabase 연결 트러블슈팅 (1순위: 인증 오류)
- **증상**: `OperationalError: FATAL: Password authentication failed`
- **원인**: 수동으로 설정한 데이터베이스 비밀번호 오입력 또는 특수문자 처리 문제.
- **해결**: Supabase 대시보드에서 비밀번호를 재설정(Reset)하고 Airflow Connection 정보에 정확히 반영합니다.

##### 2. 클라이언트 인코딩 오류 해결 (2순위: 설정 오류)
- **증상**: `server didn't return client encoding`
- **원인**: Airflow의 PostgresHook이 Supabase 서버와 통신할 때 인코딩 정보를 협상하지 못하는 현상.
- **해결(중요)**: Airflow Connection 설정의 `Extra` 필드에 아래 JSON 데이터를 추가하여 명시적으로 인코딩을 지정합니다.
  ```json
  {"client_encoding": "utf8"}
  ```

##### 3. 시니어의 디버깅 마인드셋
- **AI 활용**: 에러 로그 전체를 복사하여 AI(ChatGPT 등)에게 맥락과 함께 질문합니다.
- **검증**: AI가 제시한 해결책이 현재 환경(Supabase, Airflow 2.x 등)과 맞는지 문법과 옵션을 더블 체크합니다.

#### 💡 핵심 포인트
- 에러 로그가 바뀌는 것은 "문제가 해결되고 있는 긍정적인 신호"입니다.
- `Extra` 필드는 라이브러리 기본값 외의 특수 설정을 주입할 수 있는 강력한 도구입니다.

---

## 5교시 14:00 - 15:00

#### 📌 강의 핵심 요약
- 데이터 수집부터 적재, 알림까지 이어지는 **End-to-End(E2E) 파이프라인**을 완성합니다.
- 실시간 지하철 API 데이터를 활용하여 파이프라인의 완성도를 높이고 자동 알림 체계를 구축합니다.

#### 📚 상세 내용 정리

##### 1. E2E 파이프라인 설계 (Subway to Slack)
- **흐름**: 지하철 API 호출 → 데이터 가공 → Supabase 적재 → Slack 알림 발송.
- **Slack 알림 조건**:
  - 초기 테스트 시에는 **성공/실패 모두** 알림을 받아 연동 확인.
  - 안정화 이후에는 리소스 절약을 위해 **실패 시에만** 알림을 받는 방식으로 운영 권장.

##### 2. 운영 전략 및 성능 최적화
- **스케줄링**: 실습 부하 관리를 위해 5분 단위 스케줄을 임시 중단하거나, 테스트 시에는 `Daily` 설정 후 수동 실행을 권장합니다.
- **데이터 정합성**: 수집된 JSON 데이터가 Supabase 스케마와 일치하는지 확인하는 과정이 필수적입니다.

##### 3. [Advanced] 리전(Region) 및 네트워크 이슈
- 모든 설정이 정상임에도 발생하는 간헐적 접속 오류는 클라우드 리전 간의 일시적 라우팅 문제일 수 있습니다. 이는 시스템 레벨의 변수임을 인지하고 디버깅 시 고려해야 합니다.

#### ✅ 실습/과제
- 기존 데이터 적재 DAG 끝에 `SlackWebhookOperator`를 연결하여 통합 테스트 진행.
- Supabase 테이블에 최신 날짜 데이터가 정상 적재되었는지 SQL로 최종 확인.

---

## 6교시 15:00 - 16:00

## 7교시 16:00 - 17:00

## 8교시 17:00 - 18:00

