# Day 1 - 2026-01-05
# 🏛️ 강사 소개

## 👨‍💻 주요 이력
- **데이터 분석가 & 콘텐츠 크리에이터** (2024.04 ~ 현재)
- **n8n Global Ambassador** (2025.03 ~ 현재)
- **펄스애드** (아마존 광고 에이전시) | 데이터 분석가 (2024.11 ~ 2025.03)
- **쿠팡플레이** (OTT) | 데이터 분석가 (2022.06 ~ 2024.04)
- **카카오스타일** (이커머스) | 데이터 분석가 (2020.06 ~ 2022.06)
- **쏘카** (모빌리티) | 데이터 분석가 (2018.01 ~ 2020.04)

## 📜 자격 및 기타 활동
- **강의 경력**: 현대자동차, 애경그룹, 롯데카드, LG유플러스 등 B2B 강의 및 팀스파르타, 러닝스푼즈, 코멘토 강의 다수
- **보유 자격**:
  - 데이터분석 준전문가 (ADsP)
  - 빅데이터 분석기사 (실기)
  - Tableau Desktop Expert
- **커뮤니티**: 글또, PAP, 데분생 퍼블리셔 활동 중

## 🌐 소셜 채널
- **YouTube**: [데이터 팝콘](https://www.youtube.com/@data.popcorn)
- **LinkedIn**: [2innnnn0](https://www.linkedin.com/in/2innnnn0/)
- **Linktree**: [datapopcorn](https://linktr.ee/datapopcorn)

---

# 📖 강의 개요
본 강의는 **"AI를 잘 쓰는 데이터 분석가 및 데이터 엔지니어"** 양성을 핵심 목표로 합니다. 단순한 코드 작성을 넘어, 최신 AI 도구를 실무 파이프라인에 녹여내어 압도적인 생산성을 발휘하는 전문가로 거듭나는 과정을 다룹니다.
Let's Vibe Data Engineering!



# 📄 Google Antigravity 초보자 가이드

**공식 사이트:** [antigravity.google/product](https://antigravity.google/product)

---

## 1. 개요

**Google Antigravity**는 개발자와 제작자를 돕기 위해 설계된 차세대 **AI 기반 통합 개발 환경(IDE)**입니다.  
단순히 코드를 작성하는 도구가 아니라, **'알아서 일을 처리해주는 똑똑한 AI 비서(Agent)'**가 내장된 프로그램 제작 도구라고 이해하면 쉽습니다.

복잡한 코딩이나 작업을 AI에게 맡기고, 사용자는 **감독관**처럼 AI가 만든 결과물을 확인하고 지시를 내리는 방식으로 작업할 수 있습니다.

---

## 2. 주요 기능

이 제품의 핵심은 **'나를 대신해 일하는 AI'**와 **'그 AI를 관리하는 시스템'**입니다.

### ① 에이전트 (Agent) - '나만의 AI 직원'
- **기능**: 사용자의 명령을 받아 코드 작성, 터미널 명령 실행, 웹 브라우저 검색 등을 스스로 수행합니다.
- **특징**: 에디터, 터미널, 브라우저를 넘나들며 자율적으로 일합니다. 마치 옆에 앉은 보조 개발자처럼 행동합니다.

### ② 에디터 (Editor) - '똑똑한 작업장'
- **기능**: AI 기능이 탑재된 코딩 프로그램(IDE)입니다.
- **특징**: 일반적인 코드 편집 기능 외에도 AI에게 바로 일을 시킬 수 있는 기능들이 포함되어 있습니다.

### ③ 아티팩트 (Artifacts) - '보고서 및 결과물'
- **기능**: AI(에이전트)가 작업을 수행한 후 사용자에게 제출하는 결과물입니다.
- **특징**: AI가 무엇을 만들었는지, 작업 진행 상황은 어떤지 눈으로 확인할 수 있는 '숙제 검사물' 같은 개념입니다.

### ④ 에이전트 매니저 (Agent Manager) - '관제 센터'
- **기능**: 여러 명의 AI(에이전트)를 동시에 관리하는 화면입니다.
- **특징**: 여러 작업을 동시에 시킬 때, 각 AI가 무슨 일을 하고 있는지 한눈에 보고 통제할 수 있습니다.

### ⑤ 지식 (Knowledge) & 피드백
- **기능**: 사용자와의 대화를 통해 AI가 학습하고 기억하는 기능입니다.
- **특징**: 사용자가 결과물에 대해 *"이건 이렇게 고쳐줘"*라고 피드백(댓글)을 남기면, 이를 반영하여 다음 작업에 활용합니다.

---

## 3. 사용 방법 (워크플로우)

초보자는 다음과 같은 흐름으로 이 도구를 사용하게 됩니다.

1. **목표 설정 (Task Groups)**
   - AI에게 시킬 큰 작업을 정의합니다. (예: "비행기 추적 앱을 만들어줘")

2. **작업 지시 (Agent Action)**
   - 에이전트가 에디터와 브라우저를 오가며 코드를 짜고 필요한 정보를 찾습니다.

3. **결과 확인 (Artifacts)**
   - 에이전트가 만들어온 결과물(앱의 기능, 코드 등)을 확인합니다.

4. **피드백 및 수정 (User Feedback)**
   - 결과물이 마음에 들지 않거나 수정이 필요하면 댓글을 달듯 피드백을 줍니다. (예: "색상을 파란색으로 바꿔줘")

5. **완성**
   - AI가 피드백을 반영하여 결과물을 수정하고, 최종적으로 원하는 프로그램을 완성합니다.

---

## 4. 활용 예시

- **자동 핀볼 머신**: 로봇 공학 연구자들이 AI를 이용해 자동으로 플레이하는 핀볼 기계를 제작
- **비행기 추적 앱**: 비행 정보를 보여주는 앱 디자인 및 개발
- **협업 화이트보드 앱**: 여러 에이전트를 동시에 사용하여 기능을 빠르게 추가한 앱 개발

---

## 5. 설치 및 초기 설정

### 📥 1. 설치 방법 (Installation)
Google Antigravity는 데스크톱 애플리케이션 형태로 제공됩니다.

1. **다운로드**: [공식 홈페이지](https://antigravity.google)에 접속하여 Download 버튼을 누릅니다. (Windows/Mac/Linux 지원)
2. **설치 진행**: 다운로드한 설치 파일(.exe 또는 .dmg)을 실행합니다.
3. **초기 설정**:
   - 실행 후 Setup Flow가 시작됩니다.
   - **Import Settings**: 기존 VS Code나 Cursor 설정 가져오기 또는 새로 시작(Start fresh) 선택.
   - **Theme**: 원하는 테마(다크 모드 등) 선택.

### 🔑 2. 구글 계정 로그인 (Login)
실행 후 가장 먼저 수행해야 하는 필수 단계입니다.

1. **로그인 시작**: 초기 화면에서 'Sign in' 버튼 클릭.
2. **브라우저 인증**: 구글 로그인 창이 뜨면 계정 로그인 및 권한 허용(Allow).
3. **완료**: 인증 성공 시 앱으로 자동 전환.
   > **Tip**: 'Not eligible' 메시지가 뜨면 기업용(Workspace) 대신 **개인용 Gmail 계정**으로 시도하세요.

### 🐙 3. 깃허브(GitHub) 연동 방법

#### 방법 A: 기본 연동 (터미널/GUI)
- **터미널**: `Ctrl + ` `(백틱)으로 터미널을 열고 `git clone [저장소 주소]` 입력.
- **사이드바**: 왼쪽 Source Control 아이콘 > 'Clone Repository' 선택 > 깃허브 로그인.

#### 방법 B: 심화 연동 (MCP 활용) - 추천
AI가 내 깃허브 이슈나 코드를 직접 검색하고 이해하게 하려면 **MCP(Model Context Protocol)**를 연결해야 합니다.

1. 에디터 내 에이전트 패널 상단의 `...` (더보기) 메뉴 클릭.
2. **MCP Store** 열기.
3. 목록에서 **GitHub** 찾아 Install 클릭.
4. 화면 안내에 따라 계정 인증.

---

# 🧑‍💻 데이터 직군 이해 (Roles in Data)

2026년 데이터 시장은 단순히 분석만 하거나 파이프라인만 구축하는 것을 넘어, 각 직군의 경계가 허물어지며 새로운 역할이 중요해지고 있습니다. 특히 최근 급부상한 **Data Analytics Engineer (DAE)**에 대해 알아봅시다.

### 1. Data Analytics Engineer (DAE)란?
**DAE(Analytics Engineer)**는 데이터 엔지니어(DE)와 데이터 분석가(DA) 사이에 존재하는 '가교' 역할을 합니다. 
![](asset/AE_MEME.png)

- **핵심 목표**: 엔지니어가 수집한 로우(Raw) 데이터를 분석가가 즉시 활용할 수 있도록 깨끗하고 잘 정리된 **'분석용 데이터 모델'**로 가공하는 것.
- **주요 도구**: **dbt (data build tool)**, SQL, Data Warehouse (BigQuery, Snowflake).
- **왜 필요한가?**: 데이터 엔지니어가 구축한 복잡한 데이터 인프라와 분석가가 필요로 하는 비즈니스 인사이트 사이의 간극을 메우기 위해 등장했습니다. "데이터가 너무 지저분해서 분석을 못 하겠어요"라는 실무적 문제를 해결하며 데이터의 '신뢰성'을 책임집니다.

### 2. DA vs DE vs DS vs DAE 비교

| 직군 | 핵심 목적 | 주요 업무 | 주요 스택 |
| :--- | :--- | :--- | :--- |
| **Data Analyst (DA)** | 비즈니스 질문 해결 | 대시보드 구축, A/B 테스트, 지표 정의 | SQL, Tableau, Excel |
| **Data Engineer (DE)** | 데이터 인프라 구축 | 파이프라인(ETL) 설계, 인프라 성능 최적화 | Python, Spark, Airflow |
| **Data Scientist (DS)** | 예측 및 자동화 | ML 모델링, 통계적 추론, 실험 설계 | Python, R, PyTorch |
| **Analytics Engineer (DAE)** | 데이터 신뢰성 & 구조화 | 데이터 모델링(Modeling), 데이터 품질 관리 | dbt, SQL, BigQuery |

---

## 🎯 2026 데이터 분석가 취업 커리큘럼 (Trend 반영)


2026년 데이터 분석가에게 요구되는 핵심 역량은 **"단순 시각화를 넘어선 비즈니스 임팩트"**와 **"AI 에이전트를 활용한 초고속 분석 능력"**입니다. 데이터 뒤에 숨겨진 '맥락'을 읽고 AI를 도구로 부릴 줄 아는 분석가를 목표로 합니다.

### 1단계: 분석 기본기 (Analytical Foundation)
*데이터를 읽고 다루는 근육을 키웁니다.*
- **SQL**: 복잡한 비즈니스 로직을 쿼리로 구현 (다중 조인, 윈도우 함수, 성능 최적화)
- **Python (EDA)**: Pandas, Plotly를 활용한 데이터 탐색 및 인사이트 도출
- **통계학**: 가설 검정, 상관관계 분석, 회귀 분석 등 실무 통계 개념 이해

### 2단계: 시각화 & 비즈니스 임팩트 (Visual Storytelling)
*숫자를 가치 있는 메시지로 바꿉니다.*
- **BI Tools**: Tableau, Superset 또는 Looker Studio 활용
- **Dashboard Design**: 사용자 목적에 맞는 UI/UX 설계 및 핵심 지표(KPI) 정의
- **Storytelling**: 분석 결과를 논리적인 비즈니스 언어로 전달하는 리포트 작성

### 3단계: 프로덕트 & 그로스 분석 (Product & Growth)
*서비스의 성장을 데이터로 이끕니다.*
- **지표 설계**: 리텐션, 퍼널(Funnel) 분석, LTV 분석
- **A/B Testing**: 실험 설계, 표본 크기 산정, 통계적 유의성 판단
- **이벤트 로그 설계**: 프로덕트 개선을 위한 유저 행동 로그 설계 및 관리

### 4단계: AI 활용 분석 (AI-Driven Analytics) ⭐️
*2026년 분석가의 핵심 차별화 포인트입니다.*
- **AI Agentic EDA**: Antigravity, ChatGPT, Claude 등 AI 도구를 활용한 코드 생성 및 분석 자동화
- **프롬프트 엔지니어링**: 데이터 도메인에 특화된 프롬프트 작성 능력
- **LLM 인사이트 도출**: 비정형 데이터(리뷰, 채팅 로그)의 텍스트 마이닝 및 감성 분석 자동화 연동 파이프라인 이해

### 5단계: 실전 프로젝트
*비즈니스 문제를 실제로 해결해 본 경험입니다.*
- **추천 주제**:
    1. **"이커머스 이탈 고객 예측 및 마케팅 자동화"**: 유저 행동 분석 -> 머신러닝 모델링 -> 타겟 그룹 자동 추출
    2. **"실시간 마케팅 성과 대시보드"**: 광고 데이터 API 수집 -> 성과 지표 시각화 -> AI 기반 트렌드 분석 리포트
    3. **"데이터 기반 서비스 개선 제안서"**: 실제 앱/웹 데이터를 바탕으로 한 문제 정의 및 개선 실험 설계

---

## 🎯 2026 데이터 엔지니어링 취업 커리큘럼 (Trend 반영)

2026년 채용 시장의 핵심 키워드는 **"AI 엔지니어링과의 융합"**, **"비용 효율성(FinOps)"**, 그리고 **"데이터 거버넌스"**입니다. 신입 데이터 엔지니어에게 요구되는 역량도 단순 파이프라인 구축을 넘어, AI 모델을 위한 데이터 처리와 인프라 관리로 확장되고 있습니다.

### 1단계: 기본기 다지기 (Essential Foundation)
*흔들리지 않는 기초가 가장 중요합니다.*
- **언어**: Python (고급 문법, 비동기 처리, 타입 힌트), SQL (윈도우 함수, CTE, 쿼리 최적화)
- **컴퓨팅**: Linux/Shell Scripting, Git/GitHub 협업 흐름
- **CS 지식**: 자료구조/알고리즘, 네트워크 기초 (HTTP/API), 운영체제 기본

### 2단계: 코어 데이터 엔지니어링 (Core Engineering)
*데이터를 저장하고 가공하는 핵심 능력입니다.*
- **데이터 웨어하우스 & 모델링**:
    - Snowflake 또는 BigQuery 활용
    - 스타 스키마, 데이터 볼트(Data Vault) 모델링 이해
- **전통적 ETL vs Modern ELT**:
    - Pandas/Spark를 이용한 데이터 처리
    - **dbt (data build tool)**: SQL 기반의 데이터 변환 및 문서화 (필수 역량)



### 3단계: 모던 데이터 스택 & 인프라 (Modern Stack & Infra)
*최신 트렌드에 맞는 도구들을 익힙니다.*
- **오케스트레이션**: Apache Airflow (또는 Dagster/Prefect)
    - 복잡한 의존성 관리 및 백필(Backfill) 전략
- **컨테이너 & 클라우드**:
    - Docker & Kubernetes (기본 개념 및 간단한 배포)
    - AWS 또는 GCP (IAM, S3/GCS, Lambda/Cloud Functions, VPC)
    - **IaC**: Terraform으로 인프라 코드로 관리하기

### 4단계: 차별화 포인트 (2026 Trends) ⭐️
*경쟁자와 차별화되는 무기입니다.*
- **AI/LLM 데이터 파이프라인**:
    - **RAG (검색 증강 생성)** 아키텍처 이해
    - **Vector Database** (Pinecone, Milvus 등) 구축 및 관리 경험
    - 비정형 데이터(텍스트, 이미지) 처리 파이프라인
- **데이터 품질 및 거버넌스**:
    - 데이터 계약 (Data Contracts) 개념 이해
    - 데이터 품질 테스트 (Great Expectations, Soda)
- **실시간 데이터 처리 (Streaming)**:
    - Kafka 또는 Redpanda 기본 아키텍처
    - Spark Streaming 또는 Flink 찍먹해보기

### 5단계: 실전 프로젝트
*이력서에 들어갈 강력한 한 방입니다.*
- **추천 주제**:
    1. **"나만의 개인화 뉴스 요약 에이전트"**: 뉴스 API 수집 -> 요약(LLM) -> 벡터 DB 저장 -> 챗봇 인터페이스
    2. **"실시간 이커머스 대시보드"**: 로그 생성 -> Kafka -> 스트리밍 처리 -> 시각화
    3. **"데이터 품질 모니터링 시스템"**: 공공데이터 수집 -> 품질 검사 -> 알림 발송 -> Airflow 자동화

---

## 🎯 2026 AI/AX 전문가 취업 커리큘럼 (Trend 반영)

2026년 기업의 최우선 과제는 **"AI 도입을 넘어선 실질적인 비즈니스 전환(AX, AI Transformation)"**입니다. 단순히 AI 기술을 아는 수준을 넘어, 프로젝트 전체 아키텍처를 설계하고 생산성을 혁신할 수 있는 전문가를 지향합니다.

### 1단계: AI 기초 및 기술 이해 (AI Fundamentals)
*AI와 대화하고 통합하는 능력을 기릅니다.*
- **LLM 원리**: 주요 파운데이션 모델(GPT, Claude, Gemini)의 특성 및 한계 파악
- **프롬프트 엔지니어링 기술**: RTF, CoT(Chain of Thought), Few-shot 등 고급 기법 마스터
- **API 연동 능력**: OpenAI, Anthropic 등의 API를 활용한 서비스 프로토타이핑

### 2단계: AI 아키텍처 설계 (AI Architecture)
*데이터를 지식으로 바꾸는 인프라를 구축합니다.*
- **RAG 시스템 구축**: 지식 기반 검색 증강 생성(Retrieval-Augmented Generation) 파이프라인 설계
- **Vector Database**: Pinecone, Milvus, ChromaDB 등을 활용한 고성능 벡터 데이터 관리
- **임베딩 및 의미론적 검색**: 텍스트 데이터의 수치화 및 유사도 기반 정보 추출 최적화

### 3단계: AI 에이전트 및 자동화 (Agent & Automation)
*스스로 일하는 AI 시스템을 만듭니다.*
- **AI Agent 설계**: n8n, LangChain, LlamaIndex를 활용한 자율 에이전트 구축
- **워크플로우 자동화**: 비즈니스 프로세스에 AI를 이식하는 워크플로우 설계 (n8n Global Ambassador 수준 지향)
- **Function Calling**: AI가 외부 도구와 API를 스스로 호출하여 작업을 수행하게 하는 기술 구현

### 4단계: 엔터프라이즈 AX 최적화 (Advanced AX) ⭐️
*기업 환경에서의 AI 혁신을 이끕니다.*
- **AI 거버넌스**: 보안, 윤리, 편향성 관리 및 효율적인 비용 관리(OpenAI Token 관리 등)
- **AI 변화 관리**: 기술 도입을 위한 비즈니스 프로세스 재설계 및 변화 관리 전략
- **멀티 모달 파이프라인**: 텍스트, 이미지, 음성을 아우르는 통합 데이터 처리 인프라 운용

### 5단계: 실전 프로젝트
*실제 비즈니스 가치를 창출하는 AI 결과물입니다.*
- **추천 주제**:
    1. **"전사 지능형 지식 베이스 구축"**: 사내 문서 기반 RAG 시스템 및 슬랙/노션 연동 챗봇
    2. **"AI 기반 무인 고객 지원 에이전트"**: 단순 상담을 넘어 예약, 환불 등 실제 액션까지 수행하는 Agent
    3. **"데이터 시각화 및 자동 리포팅 에이전트"**: 자연어 명령으로 데이터를 분석하고 차트가 포함된 보고서를 생성하는 시스템

---

## 📺 추천 데이터 엔지니어링 & 분석 인플루언서 (YouTube & Blogs)

**Global (해외)**
- **[Joe Reis](https://www.linkedin.com/in/josephreis/)**: "Fundamentals of Data Engineering" 저자. 도구보다 아키텍처와 라이프사이클 중심의 철학 강조.
- **[The Seattle Data Guy](https://www.youtube.com/@SeattleDataGuy)**: 데이터 엔지니어링 커리어, 컨설팅, 모던 데이터 스택 전반을 다룸.
- **[Zach Wilson](https://www.youtube.com/@zachwilson)**: (전) 에어비앤비/넷플릭스 엔지니어. 데이터 파이프라인 설계, 데이터 모델링 및 SQL 심화 과정.
- **[Andreas Kretz](https://www.youtube.com/@AndreasKretz)**: 실무 프로젝트 중심의 엔지니어링 교육 및 플랫폼 아키텍처 조언.
- **[Tina Huang](https://www.youtube.com/@TinaHuang1)**: 효율적인 데이터 사이언스 학습법과 FAANG 취업 전략, 업무 생산성 팁 공유.
- **[Ken Jee](https://www.youtube.com/@KenJee_ds)**: 데이터 사이언스 프로젝트 포트폴리오 구축 및 실무 팁 공유.
- **[Alex The Analyst](https://www.youtube.com/@AlexTheAnalyst)**: 데이터 분석 입문자를 위한 SQL, Python, Tableau 기초 강의 및 로드맵.
- **[Krish Naik](https://www.youtube.com/@krishnaik06)**: 머신러닝, AI, 클라우드 엔지니어링 전반을 아우르는 실전 강의.
- **[Darshil Parmar](https://www.youtube.com/@DarshilParmar)**: 엔드투엔드(E2E) 데이터 엔지니어링 프로젝트 튜토리얼 최강자.
- **[Sundas Khalid](https://www.youtube.com/@SundasKhalid)**: 데이터 분석가 커리어 가이드 및 테크 업계 커리어 통찰 공유.

**Tech Blogs (필독)**
- **Netflix Tech Blog**: 데이터 엔지니어링의 표준을 제시하는 블로그.
- **Uber Eng Blog**: 대용량 트래픽 처리 및 데이터 플랫폼 아키텍처.
- **Viva Republica (Toss) Tech Blog**: 국내 최고의 데이터 플랫폼 구축 사례 공유.


------------------------------------------

# Day2 - 2026-01-06
# 🏗️ 실전 데이터 파이프라인 구축 (Public Data & Cloud DB)

Day 2에서는 공공 데이터를 수집하여 클라우드 데이터베이스에 적재하고, 이를 자동화된 파이프라인으로 연결하는 실무 과정을 다룹니다. AI 에이전트를 조력자로 활용하여 데이터 엔지니어링의 핵심 사이클을 완수합니다.

---

## 📅 타임테이블 및 상세 커리큘럼

### 1교시: 데이터 협업 및 개발 환경 (GitHub & Python Foundation)
- **주제**: 버전 관리의 기초와 데이터 엔지니어링을 위한 파이썬 환경 구축
- **내용**:
    - **GitHub 기초**: `Clone`, `Commit`, `Push` 실습 및 협업 워크플로우 이해
    - **Python Foundation**: 가상환경 설정, 패키지 관리(`pip`), 데이터 처리를 위한 핵심 문법 기초
    - Day 1 강의 노트를 본인의 레포지토리에 정리하여 커밋하기

### 2교시: 구획된 데이터와 클라우드 저장소 (SQL & Cloud Storage)
- **주제**: 관계형 데이터베이스(RDBMS)의 기초와 Supabase 설정
- **내용**:
    - **SQL 기초**: 데이터 타입, `SELECT`, `FROM`, `WHERE` 등 기본 쿼리 문법 마스터
    - **Cloud DB**: Supabase(PostgreSQL) 인스턴스 생성 및 테이블 스키마 설계 기초
    - GUI 도구를 활용한 데이터 삽입 및 조회 실습

### 3교시: 데이터 수집의 첫걸음 (Data Sourcing & API)
- **주제**: 외부 데이터를 프로그램으로 가져오는 법 (API & Requests)
- **내용**:
    - 공공데이터포털 API 연동 및 고유 인증키 보안 관리
    - `requests` 라이브러리를 활용한 API 호출 자동화 및 에러 핸들링
    - 실습: 버스/지하철 실시간 위치 데이터 소싱하기

### 4교시: 데이터 가공 및 정제 (Data Transform & Pandas)
- **주제**: '지분한' 데이터를 '쓸모 있는' 정보로 바꾸는 법
- **내용**:
    - **Pandas 기초**: DataFrame 구조 이해 및 기본 조작법
    - 데이터 클리닝: 결측치 처리, 중복 제거, 데이터 타입 최적화
    - 복잡한 JSON 구조를 평면적인 테이블(Table) 형태로 변환하기

### 5교시: 파이프라인 완성 및 적재 (Pipeline & Load)
- **주제**: 수집한 데이터를 클라우드 DB에 안전하게 밀어넣기
- **내용**:
    - `SQLAlchemy`와 `psycopg`를 활용한 파이썬-DB 연동 실습
    - **Bulk Insert**: 대량의 데이터를 한 번에 효율적으로 적재하는 방법
    - 데이터 무결성 검증 및 SQL 기반 정합성 테스트

### 6교시: AI-Native 파이프라인 최적화 (AX & Optimization)
- **주제**: AI 에이전트를 조력자로 활용한 코드 고도화
- **내용**:
    - Antigravity 에이전트에게 복잡한 예외 처리 및 로깅(Logging) 추가 지시하기
    - AI를 활용한 코드 리뷰 및 성능 최적화(Refactoring) 실습
    - 생성된 코드의 논리적 오류를 검증하는 '감독관' 역량 강화

### 7교시: 프로젝트 쇼케이스 및 자동화 (Next Steps)
- **주제**: 실습 결과 공유 및 데이터 파이프라인 자동화의 미래
- **내용**:
    - 구축된 E2E(End-to-End) 데이터 파이프라인 시연 및 회고
    - GitHub Actions를 활용한 스케줄링(Scheduling) 자동화 맛보기
    - Day 3 실전 프로젝트를 위한 환경 점검 및 Q&A

---

## 🛠️ 실습 도구 및 환경
- **IDE**: Google Antigravity
- **Language**: Python 3.10+
- **Library**: `pandas`, `requests`, `sqlalchemy`, `psycopg2-binary`
- **Database**: Supabase (PostgreSQL)
- **Collaboration**: GitHub Dashboard

---

## 💡 [초보자 가이드] 데이터베이스와 파이썬의 연결고리

초보자들에게는 **"왜 라이브러리를 하나만 쓰지 않고 여러 개를 복잡하게 쓰나요?"**가 가장 큰 의문일 수 있습니다. 우리가 Day 2에서 사용할 도구들의 역할을 아주 쉽게 비유로 풀어보겠습니다.

### 1. 전구와 어댑터: SQL Driver (`psycopg`)
*   **비유**: `PostgreSQL`이라는 **외국산 전구**를 샀는데, 우리 집 **콘센트(Python)**와 모양이 맞지 않습니다. 이때 전구를 꽂을 수 있게 해주는 **전용 어댑터**가 바로 `psycopg`입니다.
*   **왜 쓰나요?**: 파이썬은 기본적으로 데이터베이스와 대화할 줄 모릅니다. DB마다 말하는 방식이 다르기 때문에, 각 DB에 맞는 전용 통역사(드라이버)가 필요합니다.
*   **어느 상황에 쓰나요?**: 데이터베이스에 직접 접속해서 명령을 내릴 때 무조건 가장 밑바닥에 깔려 있어야 하는 필수 도구입니다.

### 2. 번역기와 비서: ORM (`SQLAlchemy`)
*   **비유**: `SQL`이라는 **외국어**를 직접 공부해서 전구 어댑터에 명령을 내릴 수도 있지만, 너무 어렵습니다. 그래서 우리는 **유능한 비서**인 `SQLAlchemy`를 고용합니다. 우리는 파이썬으로 명령을 내리고, 비서가 알아서 복잡한 SQL 외국어로 번역해서 어댑터에게 전달합니다.
*   **왜 쓰나요?**: 
    1.  **생산성**: 복잡한 SQL 쿼리를 직접 짜지 않아도 파이썬 코드로 데이터를 다룰 수 있습니다.
    2.  **안전성**: 실수로 쿼리를 잘못 짜서 데이터가 날아가는 사고를 방지해 줍니다.
    3.  **호환성**: 나중에 DB를 PostgreSQL에서 MySQL로 바꿔도, 비서(SQLAlchemy)만 있으면 코드를 거의 고치지 않아도 됩니다.
*   **어느 상황에 쓰나요?**: 실무에서 데이터를 안전하고 체계적으로 관리해야 할 때, 특히 **Pandas**와 연동해서 데이터를 DB에 한 번에 넣고 싶을 때 가장 빛을 발합니다.

### 3. 실습 요약: 우리가 하는 일
1.  **PostgreSQL (전구)**: 데이터를 저장할 실제 장소 (Supabase)
2.  **psycopg (어댑터)**: 파이썬과 DB를 전기적으로 연결해 주는 도구
3.  **SQLAlchemy (비서)**: 우리가 파이썬으로 내린 명령을 DB가 알아들을 수 있게 처리해 주는 도구
4.  **Pandas (주방장)**: 수집한 데이터를 요리하고 다듬어서 비서에게 전달하는 도구

---

# Day 3: Modern Data Stack & Pipeline Automation
> **핵심 테마**: ELT 아키텍처 이해와 dbt를 활용한 데이터 모델링, 그리고 오케스트레이션 자동화

---

## 📅 타임테이블 및 상세 커리큘럼

### 1교시: Modern Data Stack (MDS)의 이해
- **주제**: ETL에서 ELT로의 패러다임 변화와 MDS 생태계
- **내용**:
    - 전통적인 ETL과 현대적인 ELT 프로세스 비교 분석
    - Modern Data Stack의 핵심 구성 요소 (Fivetran, Airbyte, Snowflake, BigQuery, dbt)
    - 왜 지금 데이터 엔지니어에게 '데이터 모델링' 능력이 중요한가?

### 2교시: dbt를 활용한 데이터 변환 (Transformation)
- **주제**: SQL만으로 데이터 파이프라인 구축하기 (Analytics Engineering)
- **내용**:
    - dbt(data build tool)의 핵심 개념: Models, Seeds, Snapshots
    - `SELECT` 기반의 모듈화된 데이터 모델링 실습
    - 데이터 계보(Lineage) 확인 및 문서화 자동화

### 3교시: 파이프라인 오케스트레이션 기초 (Orchestration)
- **주제**: 복잡한 작업 흐름을 한눈에 관리하는 법
- **내용**:
    - 워크플로우 관리 도구의 필요성 (Airflow vs n8n vs Prefect)
    - n8n 또는 Airflow 라이트 버전을 활용한 DAG(Directed Acyclic Graph) 설계
    - 작업 간 의존성 설정 및 실패 시 알림(Slack/Email) 자동화 실습

### 4교시: 컨테이너와 클라우드 인프라 기초 (Docker)
- **주제**: "내 컴퓨터에선 되는데?" 문제를 해결하는 도커(Docker)
- **내용**:
    - Docker의 개념: Image, Container, Dockerfile
    - 파이썬 파이프라인 코드를 Docker 이미지로 빌드하고 실행하기
    - 가벼운 데이터 환경 파일럿 배포 실습

### 5교시: 실전! MDS 파이프라인 구축 프로젝트
- **주제**: dbt + Cloud DB 기반의 엔드투엔드 파이프라인 완성
- **내용**:
    - 2일차에 수집한 데이터를 dbt 모델을 통해 정제된 데이터마트(Data Mart)로 변환
    - 데이터 품질 검사(Test) 추가: `Unique`, `Not Null`, `Accepted Values`
    - 결과 데이터를 대시보드(BI)용 테이블로 최종 적재

### 6교시: AX: AI 기반 데이터 품질 관리 및 테스트 (Safety)
- **주제**: AI 에이전트를 활용한 '데이터 사고' 방지 로직 구현
- **내용**:
    - Antigravity를 활용하여 데이터 이상치(Anomaly) 감지 로직 생성 지시
    - SQL 테스트 쿼리 및 유닛 테스트 코드 자동 생성 및 적용
    - 데이터 파이프라인의 취약점 분석 및 보안 강화 가이드

### 7교시: 프로젝트 리뷰 및 Day 4 기획
- **주제**: 구축 결과 피드백 및 최종 프로젝트 주제 확정
- **내용**:
    - Day 3 실습 결과물의 코드 리뷰 및 개선점 도출
    - Day 4 'AI Agentic' 프로젝트를 위한 데이터 소스 및 아이디어 구체화
    - 다음 단계인 RAG와 AI 에이전트 연동을 위한 사전 준비

---

# Day 4: AI Agentic Engineering & Final Showcase
> **핵심 테마**: 자율형 AI 에이전트와 RAG 시스템 구축을 통한 지능형 데이터 파이프라인 완성

---

## 📅 타임테이블 및 상세 커리큘럼

### 1교시: 자율형 데이터 에이전트의 시대 (AI Agents)
- **주제**: 코딩하는 AI를 넘어 '일하는 AI' 에이전트의 이해
- **내용**:
    - Agentic Workflow의 개념: 도구 사용(Tool Use), 추론(Reasoning), 계획(Planning)
    - 데이터 엔지니어링 업무를 스스로 수행하는 에이전트 사례 분석
    - AI 에이전트와 협업하는 데이터 팀의 미래 모습

### 2교시: RAG 파이프라인과 비정형 데이터 처리
- **주제**: 흩어진 문서를 AI의 지식으로 바꾸는 기술 (RAG)
- **내용**:
    - RAG(Retrieval-Augmented Generation)의 5단계 흐름 이해
    - PDF/Notion/Web 데이터를 텍스트로 추출하고 청킹(Chunking)하기
    - 임베딩(Embedding) 모델 오픈소스 활용법 (HuggingFace, OpenAI)

### 3교시: 벡터 데이터베이스 마스터 (Vector DB)
- **주제**: AI의 기억 저장소, 고차원 데이터 관리
- **내용**:
    - Vector DB의 개념: 시맨틱 검색(Semantic Search)과 유사도 알고리즘 기초
    - Supabase pgvector 또는 Pinecone을 활용한 벡터 인덱싱 실습
    - 비정형 데이터를 DB에 저장하고 질문에 관련 답변 찾아오기

### 4교시: 데이터 거버넌스와 비용 최적화 (FinOps)
- **주제**: 안전하고 효율적인 AI 시스템 운영 노하우
- **내용**:
    - AI API 호출 비용 추적 및 최적화 전략 (Token Management)
    - 데이터 보안: 개인정보 마스킹 및 민감 정보 접근 제어
    - 지속 가능한 데이터 파이프라인의 운영(Ops) 원칙

### 5교시: [실전] 최종 캡스톤 프로젝트: 개발 및 구현
- **주제**: 나만의 AI-Native 데이터 서비스 완성하기
- **내용**:
    - 1~3일차 지식을 총동원한 개인별 엔드투엔드 파이프라인 구축
    - API 수집 → DB 적재 → dbt 변환 → RAG 연동 서비스 개발
    - 강사 및 AI 에이전트의 실시간 1:1 기술 지원

### 6교시: AI 에이전트를 활용한 최종 완성 및 고도화
- **주제**: '마지막 2%'를 채우는 에이전트 최적화
- **내용**:
    - 구현된 서비스의 성능 병목 지점 AI 진단 및 리팩토링
    - 서비스 배포를 위한 최종 패키징 및 문서화 작업
    - UI/UX 개선을 위한 에이전트 기반 프론트엔드 연동 지원

### 7교시: 프로젝트 쇼케이스 및 졸업 (Graduation)
- **주제**: 성과 공유와 앞으로의 커리어 성장을 위한 네트워킹
- **내용**:
    - 개인별 최종 프로젝트 시연 및 피드백 (쇼케이스)
    - 데이터 엔지니어 역량 로드맵 및 포트폴리오 구성 전략 가이드
    - 수료식 및 Q&A: AI 시대에 살아남는 데이터 인재의 자세

---
