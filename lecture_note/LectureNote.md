# Day 1 - 2026-01-05
# 🏛️ 강사 소개

## 👨‍💻 주요 이력
- **데이터 분석가 & 콘텐츠 크리에이터** (2024.04 ~ 현재)
- **n8n Global Ambassador** (2025.03 ~ 현재)
- **펄스애드** (아마존 광고 에이전시) | 데이터 분석가 (2024.11 ~ 2025.03)
- **쿠팡플레이** (OTT) | 데이터 분석가 (2022.06 ~ 2024.04)
- **카카오스타일** (이커머스) | 데이터 분석가 (2020.06 ~ 2022.06)
- **쏘카** (모빌리티) | 데이터 분석가 (2018.01 ~ 2020.04)

## 📜 자격 및 기타 활동
- **강의 경력**: 현대자동차, 애경그룹, 롯데카드, LG유플러스 등 B2B 강의 및 팀스파르타, 러닝스푼즈, 코멘토 강의 다수
- **보유 자격**:
  - 데이터분석 준전문가 (ADsP)
  - 빅데이터 분석기사 (실기)
  - Tableau Desktop Expert
- **커뮤니티**: 글또, PAP, 데분생 퍼블리셔 활동 중

## 🌐 소셜 채널
- **YouTube**: [데이터 팝콘](https://www.youtube.com/@data.popcorn)
- **LinkedIn**: [2innnnn0](https://www.linkedin.com/in/2innnnn0/)
- **Linktree**: [datapopcorn](https://linktr.ee/datapopcorn)

---

# 📖 강의 개요
본 강의는 **"AI를 잘 쓰는 데이터 분석가 및 데이터 엔지니어"** 양성을 핵심 목표로 합니다. 단순한 코드 작성을 넘어, 최신 AI 도구를 실무 파이프라인에 녹여내어 압도적인 생산성을 발휘하는 전문가로 거듭나는 과정을 다룹니다.
Let's Vibe Data Engineering!



# 📄 Google Antigravity 초보자 가이드

**공식 사이트:** [antigravity.google/product](https://antigravity.google/product)

---

## 1. 개요

**Google Antigravity**는 개발자와 제작자를 돕기 위해 설계된 차세대 **AI 기반 통합 개발 환경(IDE)**입니다.  
단순히 코드를 작성하는 도구가 아니라, **'알아서 일을 처리해주는 똑똑한 AI 비서(Agent)'**가 내장된 프로그램 제작 도구라고 이해하면 쉽습니다.

복잡한 코딩이나 작업을 AI에게 맡기고, 사용자는 **감독관**처럼 AI가 만든 결과물을 확인하고 지시를 내리는 방식으로 작업할 수 있습니다.

---

## 2. 주요 기능

이 제품의 핵심은 **'나를 대신해 일하는 AI'**와 **'그 AI를 관리하는 시스템'**입니다.

### ① 에이전트 (Agent) - '나만의 AI 직원'
- **기능**: 사용자의 명령을 받아 코드 작성, 터미널 명령 실행, 웹 브라우저 검색 등을 스스로 수행합니다.
- **특징**: 에디터, 터미널, 브라우저를 넘나들며 자율적으로 일합니다. 마치 옆에 앉은 보조 개발자처럼 행동합니다.

### ② 에디터 (Editor) - '똑똑한 작업장'
- **기능**: AI 기능이 탑재된 코딩 프로그램(IDE)입니다.
- **특징**: 일반적인 코드 편집 기능 외에도 AI에게 바로 일을 시킬 수 있는 기능들이 포함되어 있습니다.

### ③ 아티팩트 (Artifacts) - '보고서 및 결과물'
- **기능**: AI(에이전트)가 작업을 수행한 후 사용자에게 제출하는 결과물입니다.
- **특징**: AI가 무엇을 만들었는지, 작업 진행 상황은 어떤지 눈으로 확인할 수 있는 '숙제 검사물' 같은 개념입니다.

### ④ 에이전트 매니저 (Agent Manager) - '관제 센터'
- **기능**: 여러 명의 AI(에이전트)를 동시에 관리하는 화면입니다.
- **특징**: 여러 작업을 동시에 시킬 때, 각 AI가 무슨 일을 하고 있는지 한눈에 보고 통제할 수 있습니다.

### ⑤ 지식 (Knowledge) & 피드백
- **기능**: 사용자와의 대화를 통해 AI가 학습하고 기억하는 기능입니다.
- **특징**: 사용자가 결과물에 대해 *"이건 이렇게 고쳐줘"*라고 피드백(댓글)을 남기면, 이를 반영하여 다음 작업에 활용합니다.

---

## 3. 사용 방법 (워크플로우)

초보자는 다음과 같은 흐름으로 이 도구를 사용하게 됩니다.

1. **목표 설정 (Task Groups)**
   - AI에게 시킬 큰 작업을 정의합니다. (예: "비행기 추적 앱을 만들어줘")

2. **작업 지시 (Agent Action)**
   - 에이전트가 에디터와 브라우저를 오가며 코드를 짜고 필요한 정보를 찾습니다.

3. **결과 확인 (Artifacts)**
   - 에이전트가 만들어온 결과물(앱의 기능, 코드 등)을 확인합니다.

4. **피드백 및 수정 (User Feedback)**
   - 결과물이 마음에 들지 않거나 수정이 필요하면 댓글을 달듯 피드백을 줍니다. (예: "색상을 파란색으로 바꿔줘")

5. **완성**
   - AI가 피드백을 반영하여 결과물을 수정하고, 최종적으로 원하는 프로그램을 완성합니다.

---

## 4. 활용 예시

- **자동 핀볼 머신**: 로봇 공학 연구자들이 AI를 이용해 자동으로 플레이하는 핀볼 기계를 제작
- **비행기 추적 앱**: 비행 정보를 보여주는 앱 디자인 및 개발
- **협업 화이트보드 앱**: 여러 에이전트를 동시에 사용하여 기능을 빠르게 추가한 앱 개발

---

## 5. 설치 및 초기 설정

### 📥 1. 설치 방법 (Installation)
Google Antigravity는 데스크톱 애플리케이션 형태로 제공됩니다.

1. **다운로드**: [공식 홈페이지](https://antigravity.google)에 접속하여 Download 버튼을 누릅니다. (Windows/Mac/Linux 지원)
2. **설치 진행**: 다운로드한 설치 파일(.exe 또는 .dmg)을 실행합니다.
3. **초기 설정**:
   - 실행 후 Setup Flow가 시작됩니다.
   - **Import Settings**: 기존 VS Code나 Cursor 설정 가져오기 또는 새로 시작(Start fresh) 선택.
   - **Theme**: 원하는 테마(다크 모드 등) 선택.

### 🔑 2. 구글 계정 로그인 (Login)
실행 후 가장 먼저 수행해야 하는 필수 단계입니다.

1. **로그인 시작**: 초기 화면에서 'Sign in' 버튼 클릭.
2. **브라우저 인증**: 구글 로그인 창이 뜨면 계정 로그인 및 권한 허용(Allow).
3. **완료**: 인증 성공 시 앱으로 자동 전환.
   > **Tip**: 'Not eligible' 메시지가 뜨면 기업용(Workspace) 대신 **개인용 Gmail 계정**으로 시도하세요.

### 🐙 3. 깃허브(GitHub) 연동 방법

#### 방법 A: 기본 연동 (터미널/GUI)
- **터미널**: `Ctrl + ` `(백틱)으로 터미널을 열고 `git clone [저장소 주소]` 입력.
- **사이드바**: 왼쪽 Source Control 아이콘 > 'Clone Repository' 선택 > 깃허브 로그인.

#### 방법 B: 심화 연동 (MCP 활용) - 추천
AI가 내 깃허브 이슈나 코드를 직접 검색하고 이해하게 하려면 **MCP(Model Context Protocol)**를 연결해야 합니다.

1. 에디터 내 에이전트 패널 상단의 `...` (더보기) 메뉴 클릭.
2. **MCP Store** 열기.
3. 목록에서 **GitHub** 찾아 Install 클릭.
4. 화면 안내에 따라 계정 인증.

---

# 🧑‍💻 데이터 직군 이해 (Roles in Data)

2026년 데이터 시장은 단순히 분석만 하거나 파이프라인만 구축하는 것을 넘어, 각 직군의 경계가 허물어지며 새로운 역할이 중요해지고 있습니다. 특히 최근 급부상한 **Data Analytics Engineer (DAE)**에 대해 알아봅시다.

### 1. Data Analytics Engineer (DAE)란?
**DAE(Analytics Engineer)**는 데이터 엔지니어(DE)와 데이터 분석가(DA) 사이에 존재하는 '가교' 역할을 합니다. 
![](asset/AE_MEME.png)

- **핵심 목표**: 엔지니어가 수집한 로우(Raw) 데이터를 분석가가 즉시 활용할 수 있도록 깨끗하고 잘 정리된 **'분석용 데이터 모델'**로 가공하는 것.
- **주요 도구**: **dbt (data build tool)**, SQL, Data Warehouse (BigQuery, Snowflake).
- **왜 필요한가?**: 데이터 엔지니어가 구축한 복잡한 데이터 인프라와 분석가가 필요로 하는 비즈니스 인사이트 사이의 간극을 메우기 위해 등장했습니다. "데이터가 너무 지저분해서 분석을 못 하겠어요"라는 실무적 문제를 해결하며 데이터의 '신뢰성'을 책임집니다.

### 2. DA vs DE vs DS vs DAE 비교

| 직군 | 핵심 목적 | 주요 업무 | 주요 스택 |
| :--- | :--- | :--- | :--- |
| **Data Analyst (DA)** | 비즈니스 질문 해결 | 대시보드 구축, A/B 테스트, 지표 정의 | SQL, Tableau, Excel |
| **Data Engineer (DE)** | 데이터 인프라 구축 | 파이프라인(ETL) 설계, 인프라 성능 최적화 | Python, Spark, Airflow |
| **Data Scientist (DS)** | 예측 및 자동화 | ML 모델링, 통계적 추론, 실험 설계 | Python, R, PyTorch |
| **Analytics Engineer (DAE)** | 데이터 신뢰성 & 구조화 | 데이터 모델링(Modeling), 데이터 품질 관리 | dbt, SQL, BigQuery |

---

## 🎯 2026 데이터 분석가 취업 커리큘럼 (Trend 반영)


2026년 데이터 분석가에게 요구되는 핵심 역량은 **"단순 시각화를 넘어선 비즈니스 임팩트"**와 **"AI 에이전트를 활용한 초고속 분석 능력"**입니다. 데이터 뒤에 숨겨진 '맥락'을 읽고 AI를 도구로 부릴 줄 아는 분석가를 목표로 합니다.

### 1단계: 분석 기본기 (Analytical Foundation)
*데이터를 읽고 다루는 근육을 키웁니다.*
- **SQL**: 복잡한 비즈니스 로직을 쿼리로 구현 (다중 조인, 윈도우 함수, 성능 최적화)
- **Python (EDA)**: Pandas, Plotly를 활용한 데이터 탐색 및 인사이트 도출
- **통계학**: 가설 검정, 상관관계 분석, 회귀 분석 등 실무 통계 개념 이해

### 2단계: 시각화 & 비즈니스 임팩트 (Visual Storytelling)
*숫자를 가치 있는 메시지로 바꿉니다.*
- **BI Tools**: Tableau, Superset 또는 Looker Studio 활용
- **Dashboard Design**: 사용자 목적에 맞는 UI/UX 설계 및 핵심 지표(KPI) 정의
- **Storytelling**: 분석 결과를 논리적인 비즈니스 언어로 전달하는 리포트 작성

### 3단계: 프로덕트 & 그로스 분석 (Product & Growth)
*서비스의 성장을 데이터로 이끕니다.*
- **지표 설계**: 리텐션, 퍼널(Funnel) 분석, LTV 분석
- **A/B Testing**: 실험 설계, 표본 크기 산정, 통계적 유의성 판단
- **이벤트 로그 설계**: 프로덕트 개선을 위한 유저 행동 로그 설계 및 관리

### 4단계: AI 활용 분석 (AI-Driven Analytics) ⭐️
*2026년 분석가의 핵심 차별화 포인트입니다.*
- **AI Agentic EDA**: Antigravity, ChatGPT, Claude 등 AI 도구를 활용한 코드 생성 및 분석 자동화
- **프롬프트 엔지니어링**: 데이터 도메인에 특화된 프롬프트 작성 능력
- **LLM 인사이트 도출**: 비정형 데이터(리뷰, 채팅 로그)의 텍스트 마이닝 및 감성 분석 자동화 연동 파이프라인 이해

### 5단계: 실전 프로젝트
*비즈니스 문제를 실제로 해결해 본 경험입니다.*
- **추천 주제**:
    1. **"이커머스 이탈 고객 예측 및 마케팅 자동화"**: 유저 행동 분석 -> 머신러닝 모델링 -> 타겟 그룹 자동 추출
    2. **"실시간 마케팅 성과 대시보드"**: 광고 데이터 API 수집 -> 성과 지표 시각화 -> AI 기반 트렌드 분석 리포트
    3. **"데이터 기반 서비스 개선 제안서"**: 실제 앱/웹 데이터를 바탕으로 한 문제 정의 및 개선 실험 설계

---

## 🎯 2026 데이터 엔지니어링 취업 커리큘럼 (Trend 반영)

2026년 채용 시장의 핵심 키워드는 **"AI 엔지니어링과의 융합"**, **"비용 효율성(FinOps)"**, 그리고 **"데이터 거버넌스"**입니다. 신입 데이터 엔지니어에게 요구되는 역량도 단순 파이프라인 구축을 넘어, AI 모델을 위한 데이터 처리와 인프라 관리로 확장되고 있습니다.

### 1단계: 기본기 다지기 (Essential Foundation)
*흔들리지 않는 기초가 가장 중요합니다.*
- **언어**: Python (고급 문법, 비동기 처리, 타입 힌트), SQL (윈도우 함수, CTE, 쿼리 최적화)
- **컴퓨팅**: Linux/Shell Scripting, Git/GitHub 협업 흐름
- **CS 지식**: 자료구조/알고리즘, 네트워크 기초 (HTTP/API), 운영체제 기본

### 2단계: 코어 데이터 엔지니어링 (Core Engineering)
*데이터를 저장하고 가공하는 핵심 능력입니다.*
- **데이터 웨어하우스 & 모델링**:
    - Snowflake 또는 BigQuery 활용
    - 스타 스키마, 데이터 볼트(Data Vault) 모델링 이해
- **전통적 ETL vs Modern ELT**:
    - Pandas/Spark를 이용한 데이터 처리
    - **dbt (data build tool)**: SQL 기반의 데이터 변환 및 문서화 (필수 역량)



### 3단계: 모던 데이터 스택 & 인프라 (Modern Stack & Infra)
*최신 트렌드에 맞는 도구들을 익힙니다.*
- **오케스트레이션**: Apache Airflow (또는 Dagster/Prefect)
    - 복잡한 의존성 관리 및 백필(Backfill) 전략
- **컨테이너 & 클라우드**:
    - Docker & Kubernetes (기본 개념 및 간단한 배포)
    - AWS 또는 GCP (IAM, S3/GCS, Lambda/Cloud Functions, VPC)
    - **IaC**: Terraform으로 인프라 코드로 관리하기

### 4단계: 차별화 포인트 (2026 Trends) ⭐️
*경쟁자와 차별화되는 무기입니다.*
- **AI/LLM 데이터 파이프라인**:
    - **RAG (검색 증강 생성)** 아키텍처 이해
    - **Vector Database** (Pinecone, Milvus 등) 구축 및 관리 경험
    - 비정형 데이터(텍스트, 이미지) 처리 파이프라인
- **데이터 품질 및 거버넌스**:
    - 데이터 계약 (Data Contracts) 개념 이해
    - 데이터 품질 테스트 (Great Expectations, Soda)
- **실시간 데이터 처리 (Streaming)**:
    - Kafka 또는 Redpanda 기본 아키텍처
    - Spark Streaming 또는 Flink 찍먹해보기

### 5단계: 실전 프로젝트
*이력서에 들어갈 강력한 한 방입니다.*
- **추천 주제**:
    1. **"나만의 개인화 뉴스 요약 에이전트"**: 뉴스 API 수집 -> 요약(LLM) -> 벡터 DB 저장 -> 챗봇 인터페이스
    2. **"실시간 이커머스 대시보드"**: 로그 생성 -> Kafka -> 스트리밍 처리 -> 시각화
    3. **"데이터 품질 모니터링 시스템"**: 공공데이터 수집 -> 품질 검사 -> 알림 발송 -> Airflow 자동화

---


## 🏗️ 데이터 파이프라인 설계 원칙 (Design Principles)

데이터 엔지니어링의 핵심은 단순히 데이터를 옮기는 것이 아니라, **"안정적이고, 믿을 수 있으며, 다시 실행 가능한"** 시스템을 만드는 것입니다.

### 1. 멱등성 (Idempotency) ⭐️
- **정의**: 동일한 작업을 여러 번 수행해도 결과가 항상 같아야 함.
- **중요성**: 파이프라인이 중간에 실패하여 재실행(Rerun)할 때, 데이터가 중복되거나 꼬이는 것을 방지합니다.
- **실천**: `INSERT` 대신 `UPSERT`를 사용하거나, 적재 전 해당 범위의 데이터를 삭제(`Overwrite`)하는 전략을 사용합니다.

### 2. 재현성 (Reproducibility)
- **정의**: 원본 데이터만 있다면 언제든 과거의 특정 시점 데이터를 다시 만들어낼 수 있어야 함.
- **실천**: 로우(Raw) 데이터를 가공 없이 그대로 저장하는 영역(Bronze/Raw Layer)을 반드시 유지합니다.

### 3. 모듈화 (Modularity)
- **정의**: 수집(Extract), 변환(Transform), 적재(Load) 단계를 독립적인 컴포넌트로 분리.
- **중요성**: 특정 단계에서 에러가 났을 때 전체를 고치지 않고 해당 모듈만 수정 및 테스트할 수 있습니다.

### 4. 관측 가능성 (Observability) & 로깅
- **정의**: 파이프라인의 상태(성공, 실패, 지연 시간)와 데이터의 흐름을 한눈에 파악할 수 있어야 함.
- **실천**: Slack 알림 연동, 대시보드 활용, 상세한 에러 로그 기록을 통해 '보이지 않는 장애'를 방지합니다.

### 5. 데이터 품질 및 검증 (Data Quality & Validation)
- **원칙**: 쓰레기가 들어가면 쓰레기가 나온다 (GIGO: Garbage In, Garbage Out).
- **실천**: 적재 전 NULL 값 체크, 데이터 타입 검증, 비즈니스 로직(예: 가격은 0보다 커야 함) 검증을 자동화합니다 (예: Great Expectations).

### 6. 실패 대비 및 복구 (Fault Tolerance & Backfill)
- **정의**: 시스템은 언제든 실패할 수 있음을 가정하고 설계(Design for Failure).
- **실천**: 실패 시 자동 재시도(Retry) 로직과 과거 누락된 데이터를 채워 넣는 백필(Backfill) 프로세스를 고려합니다.

---

## 🎯 2026 AI/AX 전문가 취업 커리큘럼 (Trend 반영)

2026년 기업의 최우선 과제는 **"AI 도입을 넘어선 실질적인 비즈니스 전환(AX, AI Transformation)"**입니다. 단순히 AI 기술을 아는 수준을 넘어, 프로젝트 전체 아키텍처를 설계하고 생산성을 혁신할 수 있는 전문가를 지향합니다.

### 1단계: AI 기초 및 기술 이해 (AI Fundamentals)
*AI와 대화하고 통합하는 능력을 기릅니다.*
- **LLM 원리**: 주요 파운데이션 모델(GPT, Claude, Gemini)의 특성 및 한계 파악
- **프롬프트 엔지니어링 기술**: RTF, CoT(Chain of Thought), Few-shot 등 고급 기법 마스터
- **API 연동 능력**: OpenAI, Anthropic 등의 API를 활용한 서비스 프로토타이핑

### 2단계: AI 아키텍처 설계 (AI Architecture)
*데이터를 지식으로 바꾸는 인프라를 구축합니다.*
- **RAG 시스템 구축**: 지식 기반 검색 증강 생성(Retrieval-Augmented Generation) 파이프라인 설계
- **Vector Database**: Pinecone, Milvus, ChromaDB 등을 활용한 고성능 벡터 데이터 관리
- **임베딩 및 의미론적 검색**: 텍스트 데이터의 수치화 및 유사도 기반 정보 추출 최적화

### 3단계: AI 에이전트 및 자동화 (Agent & Automation)
*스스로 일하는 AI 시스템을 만듭니다.*
- **AI Agent 설계**: n8n, LangChain, LlamaIndex를 활용한 자율 에이전트 구축
- **워크플로우 자동화**: 비즈니스 프로세스에 AI를 이식하는 워크플로우 설계 (n8n Global Ambassador 수준 지향)
- **Function Calling**: AI가 외부 도구와 API를 스스로 호출하여 작업을 수행하게 하는 기술 구현

### 4단계: 엔터프라이즈 AX 최적화 (Advanced AX) ⭐️
*기업 환경에서의 AI 혁신을 이끕니다.*
- **AI 거버넌스**: 보안, 윤리, 편향성 관리 및 효율적인 비용 관리(OpenAI Token 관리 등)
- **AI 변화 관리**: 기술 도입을 위한 비즈니스 프로세스 재설계 및 변화 관리 전략
- **멀티 모달 파이프라인**: 텍스트, 이미지, 음성을 아우르는 통합 데이터 처리 인프라 운용

### 5단계: 실전 프로젝트
*실제 비즈니스 가치를 창출하는 AI 결과물입니다.*
- **추천 주제**:
    1. **"전사 지능형 지식 베이스 구축"**: 사내 문서 기반 RAG 시스템 및 슬랙/노션 연동 챗봇
    2. **"AI 기반 무인 고객 지원 에이전트"**: 단순 상담을 넘어 예약, 환불 등 실제 액션까지 수행하는 Agent
    3. **"데이터 시각화 및 자동 리포팅 에이전트"**: 자연어 명령으로 데이터를 분석하고 차트가 포함된 보고서를 생성하는 시스템

---

## 📺 추천 데이터 엔지니어링 & 분석 인플루언서 (YouTube & Blogs)

**Global (해외)**
- **[Joe Reis](https://www.linkedin.com/in/josephreis/)**: "Fundamentals of Data Engineering" 저자. 도구보다 아키텍처와 라이프사이클 중심의 철학 강조.
- **[The Seattle Data Guy](https://www.youtube.com/@SeattleDataGuy)**: 데이터 엔지니어링 커리어, 컨설팅, 모던 데이터 스택 전반을 다룸.
- **[Zach Wilson](https://www.youtube.com/@zachwilson)**: (전) 에어비앤비/넷플릭스 엔지니어. 데이터 파이프라인 설계, 데이터 모델링 및 SQL 심화 과정.
- **[Andreas Kretz](https://www.youtube.com/@AndreasKretz)**: 실무 프로젝트 중심의 엔지니어링 교육 및 플랫폼 아키텍처 조언.
- **[Tina Huang](https://www.youtube.com/@TinaHuang1)**: 효율적인 데이터 사이언스 학습법과 FAANG 취업 전략, 업무 생산성 팁 공유.
- **[Ken Jee](https://www.youtube.com/@KenJee_ds)**: 데이터 사이언스 프로젝트 포트폴리오 구축 및 실무 팁 공유.
- **[Alex The Analyst](https://www.youtube.com/@AlexTheAnalyst)**: 데이터 분석 입문자를 위한 SQL, Python, Tableau 기초 강의 및 로드맵.
- **[Krish Naik](https://www.youtube.com/@krishnaik06)**: 머신러닝, AI, 클라우드 엔지니어링 전반을 아우르는 실전 강의.
- **[Darshil Parmar](https://www.youtube.com/@DarshilParmar)**: 엔드투엔드(E2E) 데이터 엔지니어링 프로젝트 튜토리얼 최강자.
- **[Sundas Khalid](https://www.youtube.com/@SundasKhalid)**: 데이터 분석가 커리어 가이드 및 테크 업계 커리어 통찰 공유.

**Tech Blogs (필독)**
- **Netflix Tech Blog**: 데이터 엔지니어링의 표준을 제시하는 블로그.
- **Uber Eng Blog**: 대용량 트래픽 처리 및 데이터 플랫폼 아키텍처.
- **Viva Republica (Toss) Tech Blog**: 국내 최고의 데이터 플랫폼 구축 사례 공유.


------------------------------------------

# Day2 - 2026-01-06
# 🏗️ 실전 데이터 파이프라인 구축 (Public Data & Cloud DB)

Day 2에서는 공공 데이터를 수집하여 클라우드 데이터베이스에 적재하고, 이를 자동화된 파이프라인으로 연결하는 실무 과정을 다룹니다. AI 에이전트를 조력자로 활용하여 데이터 엔지니어링의 핵심 사이클을 완수합니다.

---

## 📅 타임테이블 및 상세 커리큘럼

### 1교시: 데이터 협업 및 개발 환경 (GitHub & Python Foundation)
- **주제**: 버전 관리의 기초와 데이터 엔지니어링을 위한 파이썬 환경 구축
- **내용**:
    - **GitHub 기초**: `Clone`, `Commit`, `Push` 실습 및 협업 워크플로우 이해
    - **Python Foundation**: 가상환경 설정, 패키지 관리(`pip`), 데이터 처리를 위한 핵심 문법 기초
    - Day 1 강의 노트를 본인의 레포지토리에 정리하여 커밋하기

### 2교시: 구획된 데이터와 클라우드 저장소 (SQL & Cloud Storage)
- **주제**: 관계형 데이터베이스(RDBMS)의 기초와 Supabase 설정
- **내용**:
    - **SQL 기초**: 데이터 타입, `SELECT`, `FROM`, `WHERE` 등 기본 쿼리 문법 마스터
    - **Cloud DB**: Supabase(PostgreSQL) 인스턴스 생성 및 테이블 스키마 설계 기초
    - GUI 도구를 활용한 데이터 삽입 및 조회 실습

### 3교시: 데이터 수집의 첫걸음 (Data Sourcing & API)
- **주제**: 외부 데이터를 프로그램으로 가져오는 법 (API & Requests)
- **내용**:
    - **[Step 1] 공공데이터포털(data.go.kr) 가입 및 준비**
        - 공식 홈페이지 접속 및 회원가입 (본인인증 필수)
        - 마이페이지 > 일반 인증키 발급 (Encoding/Decoding 방식의 차이 이해)
    - **[Step 2] 실무 데이터 활용 신청**
        - 검색창에 '[서울특별시 버스위치정보](https://www.data.go.kr/data/15000332/openapi.do)' 또는 '1. [서울시 지하철 실시간 도착정보](https://data.seoul.go.kr/dataList/OA-12764/A/1/datasetView.do#)' 2. [서울시 지하철 실시간 열차 위치정보](https://data.seoul.go.kr/dataList/OA-12601/A/1/datasetView.do) 검색
        - [활용신청] 버튼 클릭 후 사유 입력 (예: 교육용 파이프라인 구축 실습)
        - 승인 즉시 마이페이지 내 '개발계정'에서 API Key 확인 가능

        - http://ws.bus.go.kr/api/rest/buspos/getBusPosByRouteSt?serviceKey=jVwm8WYdMpGK4BqfwpN6eZvd0gaG5oJ5HihWrIhNwhaEdvvtLZHVaChaUQh8fO5GaWlRqxFROPnItSiO7%2Flf%2BA%3D%3D&startOrd=1&endOrd=10
        -  http://swopenapi.seoul.go.kr/api/subway/5a4c61745532696e393557726c696f/xml/realtimeStationArrival/0/5/%EC%84%9C%EC%9A%B8
        - http://swopenapi.seoul.go.kr/api/subway/5a4c61745532696e393557726c696f/xml/realtimePosition/0/5/1%ED%98%B8%EC%84%A0

    - **[Step 3] API 연동 및 보안 관리**
        - `requests` 라이브러리를 활용한 API 호출 자동화 및 에러 핸들링
        - API Key를 코드에 직접 노출하지 않는 환경변수(.env) 관리법 실습
    - **[Step 4] 실습**: 버스/지하철 실시간 위치 데이터를 JSON으로 파싱하여 터미널에 출력하기

### 4교시: 데이터 가공 및 정제 (Data Transform & Pandas)
- **주제**: '지분한' 데이터를 '쓸모 있는' 정보로 바꾸는 법
- **내용**:
    - **Pandas 기초**: DataFrame 구조 이해 및 기본 조작법
    - 데이터 클리닝: 결측치 처리, 중복 제거, 데이터 타입 최적화
    - 복잡한 JSON 구조를 평면적인 테이블(Table) 형태로 변환하기

### 5교시: 파이프라인 완성 및 적재 (Pipeline & Load)
- **주제**: 수집한 데이터를 클라우드 DB에 안전하게 밀어넣기
- **내용**:
    - `SQLAlchemy`와 `psycopg`를 활용한 파이썬-DB 연동 실습
    - **Bulk Insert**: 대량의 데이터를 한 번에 효율적으로 적재하는 방법
    - 데이터 무결성 검증 및 SQL 기반 정합성 테스트

### 6교시: AI-Native 파이프라인 최적화 (AX & Optimization)
- **주제**: AI 에이전트를 조력자로 활용한 코드 고도화
- **내용**:
    - Antigravity 에이전트에게 복잡한 예외 처리 및 로깅(Logging) 추가 지시하기
    - AI를 활용한 코드 리뷰 및 성능 최적화(Refactoring) 실습
    - 생성된 코드의 논리적 오류를 검증하는 '감독관' 역량 강화

### 7교시: 프로젝트 쇼케이스 및 자동화 (Next Steps)
- **주제**: 실습 결과 공유 및 데이터 파이프라인 자동화의 미래
- **내용**:
    - 구축된 E2E(End-to-End) 데이터 파이프라인 시연 및 회고
    - GitHub Actions를 활용한 스케줄링(Scheduling) 자동화 맛보기
    - Day 3 실전 프로젝트를 위한 환경 점검 및 Q&A

---

## 🛠️ 실습 도구 및 환경
- **IDE**: Google Antigravity
- **Language**: Python 3.10+
- **Library**: `pandas`, `requests`, `sqlalchemy`, `psycopg2-binary`
- **Database**: Supabase (PostgreSQL)
- **Collaboration**: GitHub Dashboard

---

## 💡 [초보자 가이드] 데이터베이스와 파이썬의 연결고리

초보자들에게는 **"왜 라이브러리를 하나만 쓰지 않고 여러 개를 복잡하게 쓰나요?"**가 가장 큰 의문일 수 있습니다. 우리가 Day 2에서 사용할 도구들의 역할을 아주 쉽게 비유로 풀어보겠습니다.

### 1. 전구와 어댑터: SQL Driver (`psycopg`)
*   **비유**: `PostgreSQL`이라는 **외국산 전구**를 샀는데, 우리 집 **콘센트(Python)**와 모양이 맞지 않습니다. 이때 전구를 꽂을 수 있게 해주는 **전용 어댑터**가 바로 `psycopg`입니다.
*   **왜 쓰나요?**: 파이썬은 기본적으로 데이터베이스와 대화할 줄 모릅니다. DB마다 말하는 방식이 다르기 때문에, 각 DB에 맞는 전용 통역사(드라이버)가 필요합니다.
*   **어느 상황에 쓰나요?**: 데이터베이스에 직접 접속해서 명령을 내릴 때 무조건 가장 밑바닥에 깔려 있어야 하는 필수 도구입니다.

### 2. 번역기와 비서: ORM (`SQLAlchemy`)
*   **비유**: `SQL`이라는 **외국어**를 직접 공부해서 전구 어댑터에 명령을 내릴 수도 있지만, 너무 어렵습니다. 그래서 우리는 **유능한 비서**인 `SQLAlchemy`를 고용합니다. 우리는 파이썬으로 명령을 내리고, 비서가 알아서 복잡한 SQL 외국어로 번역해서 어댑터에게 전달합니다.
*   **왜 쓰나요?**: 
    1.  **생산성**: 복잡한 SQL 쿼리를 직접 짜지 않아도 파이썬 코드로 데이터를 다룰 수 있습니다.
    2.  **안전성**: 실수로 쿼리를 잘못 짜서 데이터가 날아가는 사고를 방지해 줍니다.
    3.  **호환성**: 나중에 DB를 PostgreSQL에서 MySQL로 바꿔도, 비서(SQLAlchemy)만 있으면 코드를 거의 고치지 않아도 됩니다.
*   **어느 상황에 쓰나요?**: 실무에서 데이터를 안전하고 체계적으로 관리해야 할 때, 특히 **Pandas**와 연동해서 데이터를 DB에 한 번에 넣고 싶을 때 가장 빛을 발합니다.

### 3. 실습 요약: 우리가 하는 일
1.  **PostgreSQL (전구)**: 데이터를 저장할 실제 장소 (Supabase)
2.  **psycopg (어댑터)**: 파이썬과 DB를 전기적으로 연결해 주는 도구
3.  **SQLAlchemy (비서)**: 우리가 파이썬으로 내린 명령을 DB가 알아들을 수 있게 처리해 주는 도구
4.  **Pandas (주방장)**: 수집한 데이터를 요리하고 다듬어서 비서에게 전달하는 도구

---

---

## 💡 [심화 가이드] 자동화와 클라우드 인프라 (Detailed Guide)

### 1. 코드를 지키는 로봇 집사: GitHub Actions
> **정의**: GitHub 저장소에서 발생하는 이벤트(Push, PR 등)를 감지하여 자동으로 작업을 수행하는 CI/CD 플랫폼.

*   **핵심 구성 요소 (Core Components)**
    1.  **Workflow**: 자동화된 전체 프로세스 (`.yaml` 파일로 정의)
    2.  **Event**: 워크플로우를 실행시키는 방아쇠 (예: `on: push`)
    3.  **Job**: 하나의 처리 단위 (가상 머신에서 실행됨)
    4.  **Step**: Job 안에서 실행되는 개별 명령어 (스크립트 실행, 라이브러리 설치 등)

*   **기본 사용법 (Workflow 예시)**
    ```yaml
    name: Hello World Automation
    on: [push] # 코드를 올릴 때마다 실행
    jobs:
      build:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v2 # 코드 내려받기
          - name: Run a one-line script
            run: echo "Hello, World!" # 스크립트 실행
    ```

*   **🛠️ 핸즈온: 첫 번째 자동화 워크플로우 만들기**
    1.  **디렉토리 생성**: 로컬 프로젝트 루트에 `.github/workflows` 폴더를 만듭니다. (점(.)으로 시작하는 이름에 주의!)
    2.  **설정 파일 작성**: 해당 폴더 안에 `hello-actions.yml` 파일을 만들고 위의 예시 코드를 복사해 넣습니다.
    3.  **코드 전송 (Commit & Push)**: 
        ```bash
        git add .github/workflows/hello-actions.yml
        git commit -m "add: first github action"
        git push origin main
        ```
    4.  **결과 확인**: 본인의 GitHub 저장소 웹 페이지로 이동하여 상단의 **[Actions]** 탭을 클릭합니다.
    5.  **로그 검사**: 실행 중인 (또는 완료된) 'Hello World Automation' 항목을 클릭하고 `Run a one-line script` 단계의 출력값을 확인합니다.

### 2. 백엔드 종합 선물 세트: Supabase
> **정의**: 오픈소스 Firebase 대안으로, 관계형 DB(PostgreSQL)를 기반으로 하는 Backend-as-a-Service(BaaS).

*   **핵심 기능 (Key Features)**
    1.  **Database**: 강력한 PostgreSQL을 웹 대시보드에서 엑셀처럼 관리 (Table View)
    2.  **Auto-generated API**: 테이블을 만들면 즉시 RESTful API가 생성됨 (별도 백엔드 코드 불필요)
    3.  **Authentication**: 구글, 카카오 등 소셜 로그인 기능 1분 만에 연동
    4.  **Edge Functions**: 가벼운 서버 사이드 로직을 전 세계 엣지 네트워크에서 실행

*   **활용 예시 (Python 연결)**
    ```python
    from supabase import create_client
    
    url = "https://your-project.supabase.co"
    key = "your-anon-key"
    supabase = create_client(url, key)
    
    # 데이터 삽입 (SQL 없이 딕셔너리로!)
    data = supabase.table("todos").insert({"task": "Buy milk"}).execute()
    ```

*   **🛠️ 핸즈온: 클라우드 DB에 내 테이블 만들기**
    1.  **프로젝트 생성**: [Supabase 공식 홈페이지](https://supabase.com/) 로그인 후 `New Project`를 클릭합니다.
    2.  **테이블 생성**: 대시보드 왼쪽의 **[Table Editor]** -> `Create a new table`을 클릭합니다.
    3.  **데이터 입력**: 이름(`todos`)과 컬럼(`task`: text)을 설정한 후, `Insert row`를 눌러 엑셀처럼 직접 데이터를 넣어봅니다.
    4.  **연결 정보 확인**: **[Project Settings]** -> **[API]** 메뉴에서 `Project URL`과 `anon public key`를 복사합니다.
    5.  **파이썬 연결**: 위 `활용 예시` 코드에 본인의 정보(URL, Key)를 넣고 실행하여 DB에 있는 데이터를 조회해봅니다.

*   **🛠️ 핸즈온: DBeaver로 Supabase DB 연결하기 (GUI 환경)**
    
    **1) DBeaver Community 설치**
    - [공식 홈페이지](https://dbeaver.io/download/) 접속하여 본인의 OS에 맞는 버전 다운로드
    - macOS: `.dmg` 파일 설치 후 Applications 폴더로 이동
    - Windows: `.exe` 설치 파일 실행
    - Linux: `sudo snap install dbeaver-ce` 또는 `.deb`/`.rpm` 패키지 설치
    
    **2) Supabase 연결 정보 수집**
    - Supabase 대시보드에서 **[Project Settings]** → **[Database]** 이동
    - **Connection Info** 섹션에서 다음 정보 확인:
        - Host: `db.xxxxx.supabase.co`
        - Port: `5432`
        - Database: `postgres`
        - User: `postgres`
        - Password: 프로젝트 생성 시 설정한 비밀번호 (분실 시 리셋 가능)
    
    **3) DBeaver에서 새 연결 생성**
    - DBeaver 실행 후 상단의 **플러그 아이콘(New Database Connection)** 클릭
    - `PostgreSQL` 선택 → Next
    - 연결 정보 입력:
        - **Host**: Supabase에서 복사한 Host 주소
        - **Port**: `5432`
        - **Database**: `postgres`
        - **Username**: `postgres`
        - **Password**: 본인의 Supabase DB 비밀번호
        - **Show all databases** 체크 해제 (선택사항)
    - **Test Connection** 버튼 클릭하여 연결 확인
    - 처음 연결 시 PostgreSQL 드라이버 다운로드 요청이 뜨면 **Download** 클릭
    
    **4) SQL 쿼리 실행 및 데이터 탐색**
    - 왼쪽 Database Navigator에서 `postgres` → `Schemas` → `public` → `Tables` 확장
    - 본인이 만든 테이블(예: `todos`) 우클릭 → **View Data** 선택
    - 상단의 **SQL Editor** 아이콘을 눌러 직접 쿼리 작성:
        ```sql
        SELECT * FROM todos WHERE task LIKE '%milk%';
        ```
    - 쿼리 실행: `Ctrl+Enter` (Mac: `Cmd+Enter`)
    
    **5) 완료 포인트**
    - DBeaver에서 Supabase의 테이블 구조를 시각적으로 확인할 수 있나요?
    - SQL 쿼리를 직접 작성하여 데이터를 조회/수정할 수 있나요?
    - 이제 Python 코드 없이도 GUI 환경에서 데이터베이스를 관리할 수 있게 되었습니다! 🎉


---

# Day 3: Modern Data Stack & Pipeline Automation
> **핵심 테마**: ELT 아키텍처 이해와 dbt를 활용한 데이터 모델링, 그리고 오케스트레이션 자동화

---

## 📅 타임테이블 및 상세 커리큘럼

### 1교시: Modern Data Stack (MDS)의 이해
- **주제**: ETL에서 ELT로의 패러다임 변화와 MDS 생태계
- **내용**:
    - 전통적인 ETL과 현대적인 ELT 프로세스 비교 분석
    - Modern Data Stack의 핵심 구성 요소 (Fivetran, Airbyte, Snowflake, BigQuery, dbt)
    - 왜 지금 데이터 엔지니어에게 '데이터 모델링' 능력이 중요한가?

### 2교시: dbt를 활용한 데이터 변환 (Transformation)
- **주제**: SQL만으로 데이터 파이프라인 구축하기 (Analytics Engineering)
- **내용**:
    - dbt(data build tool)의 핵심 개념: Models, Seeds, Snapshots
    - `SELECT` 기반의 모듈화된 데이터 모델링 실습
    - 데이터 계보(Lineage) 확인 및 문서화 자동화
    - **[실습 프로젝트]**: [초보자를 위한 dbt 기초 - 쇼핑몰 매출 분석]
        - dbt_tutorial/AGENT.md

### 3교시: 파이프라인 오케스트레이션 기초 (Orchestration)
- **주제**: 복잡한 작업 흐름을 한눈에 관리하는 법
- **내용**:
    - 워크플로우 관리 도구의 필요성 (Airflow vs n8n vs Prefect)
    - n8n 또는 Airflow 라이트 버전을 활용한 DAG(Directed Acyclic Graph) 설계
    - 작업 간 의존성 설정 및 실패 시 알림(Slack/Email) 자동화 실습

### 4교시: 컨테이너와 클라우드 인프라 기초 (Docker)
- **주제**: "내 컴퓨터에선 되는데?" 문제를 해결하는 도커(Docker)
- **내용**:
    - Docker의 개념: Image, Container, Dockerfile
    - 파이썬 파이프라인 코드를 Docker 이미지로 빌드하고 실행하기
    - 가벼운 데이터 환경 파일럿 배포 실습

### 5교시: 실전! MDS 파이프라인 구축 프로젝트
- **주제**: dbt + Cloud DB 기반의 엔드투엔드 파이프라인 완성
- **내용**:
    - 2일차에 수집한 데이터를 dbt 모델을 통해 정제된 데이터마트(Data Mart)로 변환
    - 데이터 품질 검사(Test) 추가: `Unique`, `Not Null`, `Accepted Values`
    - 결과 데이터를 대시보드(BI)용 테이블로 최종 적재

### 6교시: AX: AI 기반 데이터 품질 관리 및 테스트 (Safety)
- **주제**: AI 에이전트를 활용한 '데이터 사고' 방지 로직 구현
- **내용**:
    - Antigravity를 활용하여 데이터 이상치(Anomaly) 감지 로직 생성 지시
    - SQL 테스트 쿼리 및 유닛 테스트 코드 자동 생성 및 적용
    - 데이터 파이프라인의 취약점 분석 및 보안 강화 가이드

### 7교시: 프로젝트 리뷰 및 Day 4 기획
- **주제**: 구축 결과 피드백 및 최종 프로젝트 주제 확정
- **내용**:
    - Day 3 실습 결과물의 코드 리뷰 및 개선점 도출
    - Day 4 'AI Agentic' 프로젝트를 위한 데이터 소스 및 아이디어 구체화
    - 다음 단계인 RAG와 AI 에이전트 연동을 위한 사전 준비

---

---

## 💡 [심화 가이드] 데이터 흐름 제어와 비즈니스 지표 (Detailed Guide)

### 1. 데이터 파이프라인의 관제탑: Airflow
> **정의**: 에어비앤비(Airbnb)에서 만든, 복잡한 데이터 파이프라인을 코드로 작성(Python)하고 스케줄링 및 모니터링하는 플랫폼.

*   **핵심 개념 (Key Concepts)**
    1.  **DAG (Directed Acyclic Graph)**: 작업의 흐름도. "방향성(Directed)은 있지만 순환(Cycle)하지 않는 그래프".
    2.  **Operator**: 실제 작업을 수행하는 일꾼 (예: `PythonOperator`, `BashOperator`).
    3.  **Task**: Operator가 실행될 때의 인스턴스.
    4.  **Scheduler**: 정해진 시간에 DAG를 실행시키는 알람 시계.

*   **코드 구조 맛보기 (Python)**
    ```python
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    
    with DAG('my_pipeline', schedule_interval='@daily') as dag:
        # 작업 1: 데이터 추출
        t1 = PythonOperator(task_id='extract', python_callable=extract_data)
        # 작업 2: 데이터 적재
        t2 = PythonOperator(task_id='load', python_callable=load_data)
        
        t1 >> t2 # 작업 순서 정의 (t1이 끝나야 t2 실행)
    ```

*   **🛠️ 핸즈온: 나의 첫 번째 DAG 실행하기**
    1.  **환경 준비**: 에이전트와 함께 만들어둔 `airflow_tutorial` 폴더 또는 본인의 가상환경으로 이동합니다.
    2.  **스케줄러 켜기**: 터미널에서 `airflow scheduler` 명령어를 입력해 알람 시계를 켭니다.
    3.  **대시보드 접속**: 새 터미널 창을 열어 `airflow webserver`를 실행하고 `localhost:8080`에 접속합니다.
    4.  **작업 활성화**: 우리가 만든 `bitcoin_tracker_v1` 좌측의 토글 스위치를 **[ON]**으로 바꿉니다.
    5.  **성공 확인**: 그래프 뷰(Graph View)에서 사각형 색깔이 **검은색(Running)**에서 **녹색(Success)**으로 변하는지 지켜봅니다.

### 2. 숫자의 단일 진실 공급원: Metric Store
> **정의**: 비즈니스 지표(Metrics)를 코드(YAML 등)로 한 번만 정의하여, 모든 데이터 도구(BI, SQL, Python)에서 동일한 값을 조회하도록 보장하는 계층.

*   **왜 필요한가? (The Problem)**
    *   영업팀: "매출 = 판매액"
    *   재무팀: "매출 = 판매액 - 환불액"
    *   **결과**: 회의 시간마다 "누구 숫자가 맞아?"라고 싸우게 됨.

*   **해결책 (The Solution)**
    *   `Metric Store`에 "매출"을 정의하면, 누구나 똑같은 로직으로 계산된 값을 가져갑니다.
    *   **dbt Semantic Layer**가 대표적인 예시입니다.

*   **정의 예시 (YAML)**
    ```yaml
    metric:
      name: revenue
      label: "Total Revenue"
      type: simple
      type_params:
        measure: total_sales_usd # 여기서 한 번 정의하면 끝!
    ```

*   **🛠️ 핸즈온: 지표 정의 연습하기**
    1.  **문제 상황**: 마케팅팀은 "클릭률(CTR) = 클릭 / 노출"이라고 말하고, 광고팀은 "클릭률 = 고유 클릭 / 노출"이라고 주장합니다.
    2.  **지표 통합**: 공통의 `analytics.yml` 파일을 만들고 아래와 같이 공식적으로 정의합니다.
    3.  **실습**: `metric.name: ctr`, `measure: unique_clicks`와 같이 본인이 생각하는 올바른 정의를 YAML 형식으로 직접 적어봅니다.
    4.  **검증**: 이렇게 정의된 파일이 있으면, 어떤 팀이든 이 파일만 참조하면 항상 같은 CTR 값을 얻게 됨을 이해합니다.

*   **[실습 프로젝트]**: [초보자를 위한 Metric Store 기초 - 전사 매출 지표 통일]metric_store_tutorial/AGENT.md

---

## 💡 [심화 가이드] 알림과 협업 자동화 (Detailed Guide)

### 4. 실시간 알림의 핵심: Slack API 연동
> **정의**: 데이터 파이프라인의 성공/실패, 이상치 발견 등 중요한 이벤트를 팀 채널로 자동 전송하는 협업 도구 연동 기술.

*   **비유 (Metaphor)**: 밤새 돌아가는 데이터 파이프라인이 문제를 만나면, 아침에 출근해서야 알게 되면 늦습니다. `Slack API`는 문제가 생기는 즉시 담당자의 핸드폰에 **비상벨**을 울려주는 시스템입니다.

*   **연동 방법 2가지**
    
    **1) Incoming Webhook (가장 간단)**
    - 한 줄의 URL만으로 메시지 전송 가능 (인증 불필요)
    - 특정 채널에만 메시지를 보낼 수 있음
    
    **Step-by-Step:**
    1. Slack 워크스페이스에서 `앱 추가` → `Incoming Webhooks` 검색
    2. 채널 선택 후 Webhook URL 복사 (예: `https://hooks.slack.com/services/T00/B00/XXX`)
    3. Python 코드로 메시지 전송:
    ```python
    import requests
    
    webhook_url = "your-webhook-url"
    message = {"text": "🚨 데이터 파이프라인 실행 완료!"}
    requests.post(webhook_url, json=message)
    ```
    
    **2) Bot Token (고급 기능 활용)**
    - 사용자 멘션, 파일 업로드, 메시지 수정 등 다양한 기능 사용 가능
    - 여러 채널에 동적으로 메시지를 보낼 수 있음
    
    **Step-by-Step:**
    1. Slack App 생성 (`api.slack.com/apps` → Create New App)
    2. `OAuth & Permissions`에서 `chat:write` 권한 추가
    3. 워크스페이스에 앱 설치 후 **Bot User OAuth Token** 복사
    4. Python SDK 사용:
    ```python
    from slack_sdk import WebClient
    
    client = WebClient(token="xoxb-your-bot-token")
    client.chat_postMessage(
        channel="#data-alerts",
        text="✅ 일일 매출 데이터 적재 완료! 총 1,234건"
    )
    ```

*   **실무 활용 예시**
    - **Airflow DAG 실패 알림**: 특정 Task가 3번 재시도 후에도 실패하면 Slack으로 에러 로그 전송
    - **Great Expectations 검증 실패**: 데이터 품질 검사에서 임계값을 초과하면 즉시 알림
    - **일일 리포트 자동 전송**: 매일 오전 9시에 전날 집계된 핵심 지표를 팀 채널에 공유

---

---

## 💡 [심화 가이드] 데이터 신뢰성과 품질 관리 (Detailed Guide)

### 3. 데이터 건강검진: Great Expectations (GX)
> **정의**: 데이터가 우리가 기대하는 원칙(Expectations)을 따르고 있는지 자동으로 검사하고 결과 보고서(Data Docs)를 만들어주는 오픈소스 데이터 품질 관리 도구.

*   **비유 (Metaphor)**: 우리 몸의 이상을 미리 발견하기 위해 정기 검진을 받는 것처럼, 데이터도 파이프라인 안에서 병들지 않았는지(비정상적인 값이 들어오지 않았는지) `Great Expectations`라는 **데이터 건강검진 센터**를 거쳐야 합니다.

*   **핵심 개념 (Key Concepts)**
    1.  **Expectations**: 데이터에 대한 '기대값' (예: "ID 컬럼에는 빈 값이 없어야 해!", "나이는 0보다 커야 해!")
    2.  **Validate**: 실제 데이터를 기대값과 대조하여 합격/불합격을 판정하는 과정.
    3.  **Data Docs**: 검사 결과를 누구나 이해하기 쉽게 시각화된 HTML 문서로 생성.
    4.  **Checkpoint**: 데이터 수집 직후나 DB 적재 전 등 특정 시점에서 검사를 실행하는 검문소.

*   **코드 예시 (Python)**
    ```python
    import great_expectations as gx

    # 1. 데이터 소스 연결 (예: pandas dataframe)
    df_ge = gx.from_pandas(your_dataframe)

    # 2. '기대치' 설정하기 (ID는 유효해야 하고, 가격은 0 이상이어야 함)
    df_ge.expect_column_values_to_not_be_null("order_id")
    df_ge.expect_column_values_to_be_between("amount", min_value=0)

    # 3. 검사 결과 확인
    results = df_ge.validate()
    print(results.success) # True/False
    ```

*   **주요 테스트 유형 (Common Expectations)**
    
    1.  **컬럼 존재성 검사**: 테이블에 필요한 컬럼이 있는지, 전체 컬럼 개수가 맞는지 확인
    2.  **NULL 값 검사**: 필수 필드는 빈 값이 없는지, 선택 필드는 NULL 허용 여부 확인
    3.  **데이터 타입 검사**: 각 컬럼이 정해진 데이터 타입(int, float, string 등)을 따르는지 확인
    4.  **값의 범위 검사**: 숫자 데이터가 합리적인 범위(0~100점, 최저/최고 온도 등) 안에 있는지
    5.  **값의 집합 검사**: 상태 코드나 카테고리가 미리 정의된 값들만 가지는지 (예: "대기중", "완료", "취소")
    6.  **유일성 검사**: Primary Key처럼 중복되면 안 되는 컬럼에 중복 값이 없는지
    7.  **정규식 패턴 검사**: 이메일, 전화번호 등이 올바른 형식을 따르는지 (regex로 검증)
    8.  **통계적 이상치 검사**: 평균값, 표준편차, 분산 등이 정상 범위를 벗어나지 않았는지


*   **🛠️ 핸즈온: 나의 데이터 건강검진 리포트 만들기**
    1.  **설치**: `pip install great_expectations`
    2.  **초기화**: 터미널에서 `great_expectations init` 입력 (자동으로 필요한 폴더 구조가 생성됩니다.)
    3.  **데이터 연결**: 본인의 CSV 또는 DB 데이터를 GX에 연결합니다.
    4.  **검사 규칙 생성**: "날짜 컬럼이 비어있지 않은가?"와 같은 간단한 규칙을 추가해 봅니다.
    5.  **리포트 확인**: `great_expectations docs build`를 실행하여 브라우저에서 환상적인 **데이터 품질 통계 리포트**를 확인해 보세요!

---

---

## 💡 [심화 가이드] 클라우드 플랫폼 기초 (Cloud Platform Basics)

### 5. AWS 기초 셋팅 (Amazon Web Services)
> **정의**: 전 세계에서 가장 많이 사용되는 클라우드 컴퓨팅 플랫폼으로, 데이터 저장, 연산, 네트워킹 등 200개 이상의 서비스를 제공합니다.

*   **핵심 서비스 3가지 (Data Engineering 필수)**
    
    **1) IAM (Identity and Access Management) - 권한 관리**
    - 사용자, 그룹, 역할을 생성하여 AWS 리소스에 대한 접근 제어
    - **실습 Step:**
        1. AWS Console 로그인 → `IAM` 검색
        2. Users → `Add User` → 이름 입력 (예: `data-engineer-user`)
        3. `Attach policies directly` → `AmazonS3FullAccess` 선택
        4. 사용자 생성 후 `Security credentials` → `Create access key`
        5. **Access Key ID**와 **Secret Access Key** 안전하게 저장 (한 번만 표시됨!)
    
    **2) S3 (Simple Storage Service) - 객체 스토리지**
    - 무제한 용량의 파일 저장소 (CSV, JSON, Parquet 등 데이터 파일 보관)
    - **실습 Step:**
        1. AWS Console → `S3` 검색 → `Create bucket`
        2. Bucket 이름 입력 (전 세계에서 유일해야 함, 예: `my-data-lake-2026`)
        3. Region 선택 (예: `ap-northeast-2` = 서울)
        4. `Block all public access` 활성화 (보안 필수!)
        5. Python으로 파일 업로드:
        ```python
        import boto3
        s3 = boto3.client('s3', 
            aws_access_key_id='YOUR_ACCESS_KEY',
            aws_secret_access_key='YOUR_SECRET_KEY')
        s3.upload_file('local_file.csv', 'my-data-lake-2026', 'data/file.csv')
        ```
    
    **3) Lambda - 서버리스 함수**
    - 서버 관리 없이 코드만 실행하는 환경 (API 호출, 데이터 변환 등에 활용)
    - **실습 Step:**
        1. AWS Console → `Lambda` 검색 → `Create function`
        2. `Author from scratch` 선택
        3. Function 이름 입력 → Runtime은 `Python 3.x` 선택
        4. 코드 편집기에서 간단한 함수 작성:
        ```python
        def lambda_handler(event, context):
            return {'statusCode': 200, 'body': 'Hello from Lambda!'}
        ```
        5. `Test` 버튼으로 실행 결과 확인

*   **비용 관리 TIP**: AWS Free Tier를 활용하면 S3 5GB, Lambda 100만 건 요청/월까지 무료!

---

### 6. GCP 기초 셋팅 (Google Cloud Platform)
> **정의**: Google이 제공하는 클라우드 플랫폼으로, BigQuery와 같은 강력한 데이터 분석 도구와 AI/ML 서비스로 유명합니다.

*   **핵심 서비스 3가지 (Data Engineering 필수)**
    
    **1) IAM (Identity and Access Management) - 서비스 계정**
    - GCP 리소스에 접근하기 위한 인증 정보 생성
    - **실습 Step:**
        1. [GCP Console](https://console.cloud.google.com/) 로그인 → 프로젝트 생성
        2. `IAM & Admin` → `Service Accounts` → `Create Service Account`
        3. 계정 이름 입력 (예: `data-pipeline-sa`)
        4. Role 선택: `Storage Admin` (GCS 전체 권한)
        5. `Create Key` → JSON 다운로드 (이 파일로 인증!)
        6. 환경변수 설정:
        ```bash
        export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
        ```
    
    **2) GCS (Google Cloud Storage) - 객체 스토리지**
    - AWS S3와 유사한 파일 저장소
    - **실습 Step:**
        1. GCP Console → `Cloud Storage` → `Create Bucket`
        2. Bucket 이름 입력 (예: `my-gcp-data-lake`)
        3. Location type: `Region` → `asia-northeast3` (서울)
        4. `gsutil` CLI로 파일 업로드:
        ```bash
        gsutil cp local_file.csv gs://my-gcp-data-lake/data/
        ```
        5. Python으로 접근:
        ```python
        from google.cloud import storage
        client = storage.Client()
        bucket = client.bucket('my-gcp-data-lake')
        blob = bucket.blob('data/file.csv')
        blob.upload_from_filename('local_file.csv')
        ```
    
    **3) Cloud Functions - 서버리스 함수**
    - HTTP 요청이나 Pub/Sub 메시지로 트리거되는 함수
    - **실습 Step:**
        1. GCP Console → `Cloud Functions` → `Create Function`
        2. Function 이름 입력 → Trigger type: `HTTP`
        3. Runtime: `Python 3.x` 선택
        4. `main.py`에 코드 작성:
        ```python
        def hello_gcp(request):
            return 'Hello from GCP Cloud Functions!'
        ```
        5. Entry point: `hello_gcp` 입력 → Deploy
        6. 생성된 URL로 브라우저 접속하여 결과 확인

*   **비용 관리 TIP**: GCP도 $300 크레딧을 제공하며, Cloud Functions는 월 200만 건까지 무료!

--- 
## 2026-01-08
### 7. Docker 기초 & Airflow 환경 구축 가이드 (Docker & Airflow Setup)
> **정의**: 애플리케이션을 실행하기 위한 모든 환경(OS, 라이브러리, 설정 등)을 하나의 "컨테이너"에 담아 배포하는 기술.

*   **비유 (Metaphor)**:
    - **내 컴퓨터 (로컬)**: 집마다 주방 기구와 가스레인지가 다 달라서 요리법이 달라짐.
    - **Docker (밀키트/도시락)**: 재료와 조리 도구가 완벽하게 갖춰진 "도시락"을 배달함. 전자레인지(Docker 엔진)만 있으면 어디서든 똑같은 맛의 요리(프로그램)를 먹을 수 있음.
    - **왜 쓰나요?**: "제 컴퓨터에서는 되는데요?"라는 변명을 없애줍니다(서버 환경의 통일).

*   **Step 1: Docker Desktop 설치 (Installation)**
    1.  **다운로드**: [Docker 공식 홈페이지](https://www.docker.com/products/docker-desktop/) 접속 → OS(Mac with Apple Silicon/Intel, Windows)에 맞게 다운로드.
    2.  **설치**: 다운로드한 파일(`.dmg` or `.exe`) 실행. (Mac은 아이콘을 Applications 폴더로 드래그)
    3.  **실행**: Docker Desktop 앱 실행. (고래 아이콘이 상태 표시줄에 뜨고 초록색 불이 들어와야 정상)
    
*   **Step 2: 설치 확인 (Verification)**
    - 터미널(Terminal)을 열고 다음 명령어 입력:
    ```bash
    docker --version
    # Docker version 20.10.xx, build xxx... (버전 정보가 뜨면 성공)
    
    docker run hello-world
    # "Hello from Docker!" 메시지가 뜨면 컨테이너 실행 성공
    ```

*   **Step 3: Apache Airflow 이미지 가져오기 (Pulling Image)**
    - Airflow는 복잡한 설치 과정 없이 Docker 이미지를 가져오면 바로 실행할 수 있습니다.
    - **이미지 다운로드**:
    ```bash
    docker pull apache/airflow
    # 다운로드가 완료될 때까지 대기
    ```
    - **이미지 확인**:
    ```bash
    docker images
    # REPOSITORY       TAG       IMAGE ID       SIZE
    # apache/airflow   latest    xxxxxxxxx      1.2GB (목록에 보이면 성공)
    ```

*   **Step 4: 맛보기 실행 (Simple Run)**
    - 복잡한 설정(`docker-compose`) 없이 Airflow가 켜지는지 확인만 해봅니다.
    ```bash
    docker run -p 8080:8080 apache/airflow standalone
    ```
    - 실행 후 브라우저에서 `localhost:8080` 접속.
    - 터미널에 출력된 `username`과 `password`로 로그인.

---


# Day 4: AI Agentic Engineering & Final Showcase
> **핵심 테마**: 자율형 AI 에이전트와 RAG 시스템 구축을 통한 지능형 데이터 파이프라인 완성

---

## 📅 타임테이블 및 상세 커리큘럼

### 1교시: 자율형 데이터 에이전트의 시대 (AI Agents)
- **주제**: 코딩하는 AI를 넘어 '일하는 AI' 에이전트의 이해
- **내용**:
    - Agentic Workflow의 개념: 도구 사용(Tool Use), 추론(Reasoning), 계획(Planning)
    - 데이터 엔지니어링 업무를 스스로 수행하는 에이전트 사례 분석
    - AI 에이전트와 협업하는 데이터 팀의 미래 모습

### 2교시: RAG 파이프라인과 비정형 데이터 처리
- **주제**: 흩어진 문서를 AI의 지식으로 바꾸는 기술 (RAG)
- **내용**:
    - RAG(Retrieval-Augmented Generation)의 5단계 흐름 이해
    - PDF/Notion/Web 데이터를 텍스트로 추출하고 청킹(Chunking)하기
    - 임베딩(Embedding) 모델 오픈소스 활용법 (HuggingFace, OpenAI)

### 3교시: 벡터 데이터베이스 마스터 (Vector DB)
- **주제**: AI의 기억 저장소, 고차원 데이터 관리
- **내용**:
    - Vector DB의 개념: 시맨틱 검색(Semantic Search)과 유사도 알고리즘 기초
    - Supabase pgvector 또는 Pinecone을 활용한 벡터 인덱싱 실습
    - 비정형 데이터를 DB에 저장하고 질문에 관련 답변 찾아오기

### 4교시: 데이터 거버넌스와 비용 최적화 (FinOps)
- **주제**: 안전하고 효율적인 AI 시스템 운영 노하우
- **내용**:
    - AI API 호출 비용 추적 및 최적화 전략 (Token Management)
    - 데이터 보안: 개인정보 마스킹 및 민감 정보 접근 제어
    - 지속 가능한 데이터 파이프라인의 운영(Ops) 원칙

### 5교시: [실전] 최종 캡스톤 프로젝트: 개발 및 구현
- **주제**: 나만의 AI-Native 데이터 서비스 완성하기
- **내용**:
    - 1~3일차 지식을 총동원한 개인별 엔드투엔드 파이프라인 구축
    - API 수집 → DB 적재 → dbt 변환 → RAG 연동 서비스 개발
    - 강사 및 AI 에이전트의 실시간 1:1 기술 지원

### 6교시: AI 에이전트를 활용한 최종 완성 및 고도화
- **주제**: '마지막 2%'를 채우는 에이전트 최적화
- **내용**:
    - 구현된 서비스의 성능 병목 지점 AI 진단 및 리팩토링
    - 서비스 배포를 위한 최종 패키징 및 문서화 작업
    - UI/UX 개선을 위한 에이전트 기반 프론트엔드 연동 지원

### 7교시: 프로젝트 쇼케이스 및 졸업 (Graduation)
- **주제**: 성과 공유와 앞으로의 커리어 성장을 위한 네트워킹
- **내용**:
    - 개인별 최종 프로젝트 시연 및 피드백 (쇼케이스)
    - 데이터 엔지니어 역량 로드맵 및 포트폴리오 구성 전략 가이드
    - 수료식 및 Q&A: AI 시대에 살아남는 데이터 인재의 자세

---
