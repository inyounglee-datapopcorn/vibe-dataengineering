# 📝 2026-01-12

## 1교시 09:00 - 10:00

#### 📌 강의 핵심 요약
이 강의에서는 개발 생산성을 높이기 위한 도구 숙련도와 OS의 접근성 기능(화면 확대, 텍스트 표시) 활용법을 다룹니다. 또한, 다수의 API 키와 비밀번호를 안전하게 관리하기 위해 1Password, Okta와 같은 비밀번호 관리 도구의 필요성을 강조하며, 보안을 위해 비밀번호는 난수로 생성하고 MFA(다중 인증)를 적극적으로 활용할 것을 권장합니다.

#### 📚 상세 내용 정리

##### 1. 도구 숙련도와 접근성 기능 (Accessibility)
- **개념 설명**: 개발자는 단축키와 OS 기능을 능숙하게 다루어 생산성을 높여야 합니다. 특히 '손쉬운 사용(Accessibility)' 기능은 시각적 보조뿐만 아니라 협업 시 커뮤니케이션에도 유용합니다.
- **예시/실습 내용**:
  - **화면 확대(Zoom)**: `Ctrl` + 스크롤 (또는 OS별 설정)을 통해 화면의 특정 영역을 확대합니다.
  - **텍스트 표시**: `Command`키를 누른 채 텍스트 위에 마우스를 올리면 해당 텍스트를 크게 팝업으로 보여주는 기능입니다. (Mac '손쉬운 사용' > '텍스트 표시' 설정)
- **주의사항 또는 팁**:
  - 내 모니터에서는 잘 보이지만, 뒤에서 보는 동료나 발표 화면에서는 작게 보일 수 있습니다. 이 기능을 활용해 정확한 정보 전달이 가능합니다.

##### 2. 비밀번호 및 키 관리 (Password Management)
- **개념 설명**: 수백 개의 API 키와 계정 비밀번호를 안전하게 관리하기 위해 전용 관리 도구(Password Manager)를 사용해야 합니다.
- **예시/실습 내용**:
  - **추천 도구**:
    - **1Password**: 유료지만 강력한 기능을 제공하며 가족 공유가 가능합니다.
    - **Okta Personal**: 개인 사용자에게 무료로 제공되는 관리 도구입니다.
- **주의사항 또는 팁**:
  - 비밀번호는 절대 외우지 않고 **난수(Random String)**로 생성하여 관리 도구에 저장합니다.
  - 계정 털림 방지를 위해 MFA(Multi-Factor Authentication)를 반드시 활성화합니다.

#### 💡 핵심 포인트
- "도구 숙련도는 단축키 하나가 아주 큽니다."
- "비밀번호는 외우는 것이 아니라, 관리 도구에 맡기고 잃어버리면 안 되는 존재로 관리해야 합니다."
- "협업 시 동료가 내 화면을 잘 볼 수 있도록 배려하는 것이 중요합니다."

#### ✅ 실습/과제
- 개인적으로 수/목/금 시간을 활용하여 Github Actions 내용 실습하기.
- OS의 '손쉬운 사용' 메뉴에서 화면 확대 및 텍스트 표시 기능 설정해보기.

#### 🔗 관련 키워드
- Github Actions, Accessibility(손쉬운 사용), Zoom, Password Manager, 1Password, Okta, MFA, Random Password


## 2교시 10:00 - 11:00

#### 📌 강의 핵심 요약
신사업 부서의 채용 특성과 분석 도구(Tableau, Redash)의 실무적 특징을 다룹니다. 신사업 부서는 빠른 성장을 위해 채용 절차를 신속하게 진행하는 경향이 있어 진입 기회가 될 수 있으며, 데이터 시각화 도구인 태블로(Tableau)의 리소스 사용 특성과 레거시로 인한 한계점 등을 공유합니다.

#### 📚 상세 내용 정리

##### 1. 신사업 부서 채용 및 커리어 전략
- **개념 설명**: 쿠팡, 토스 등 대기업의 신사업(New Business) 부서는 인력 충원이 급격히 필요한 경우가 많아 채용 문턱이 상대적으로 유동적일 수 있습니다.
- **예시/실습 내용**:
  - 강사의 쿠팡 플레이(Coupang Play) 입사 사례: 이미 다른 회사에 합격한 상태였으나, 쿠팡 측의 빠른 대응(1주일 내 3차 면접 완료, 익일 결과 통보)으로 입사하게 됨.
- **주의사항 또는 팁**:
  - 회사가 급한 상황(눈치싸움)을 캐치하는 것도 역량이며, 신사업 부서는 기존 시스템의 부채가 적어 상대적으로 빠르게 업무에 적응할 수 있는 장점이 있습니다.

##### 2. 데이터 분석 및 시각화 도구 활용 (Tableau vs. Redash)
- **개념 설명**: 실무에서는 대시보드 구축을 위해 Redash나 Tableau를 주로 사용합니다.
- **예시/실습 내용**:
  - **Redash**: Supabase(클라우드 DB)와 연결하여 실무에서 가볍게 활용하기 좋음.
  - **Tableau**: 고성능 시각화를 제공하지만 '무겁다'는 단점이 있음. 내부적으로 GPU를 사용하여 3D 렌더링 방식의 인터랙티브 차트를 구현함.
- **주의사항 또는 팁**:
  - 태블로는 20년 가까운 역사를 가진 도구로 레거시(Legacy) 이슈가 많습니다.
  - 특히 맥(Mac) 버전에서 발생하는 특정 버그들이 있으나, 재현 불가능으로 해결되지 않는 경우가 많으므로 실무 활용 시 유의가 필요합니다.

#### 💡 핵심 포인트
- "회사가 급하면 일단 뽑습니다. 이 기회를 캐치하는 것이 실력만큼 중요합니다."
- "태블로는 예쁘지만 리소스를 많이 소모하며, 오래된 툴이라 버그 이슈가 있을 수 있습니다."
- "실무 분석에서는 도구의 화려함보다 데이터의 정확성과 전달 속도가 중요합니다."

#### ✅ 실습/과제
- Redash와 Supabase를 연결하여 데이터 시각화 기초 실습하기.

#### 🔗 관련 키워드
- 신사업, 쿠팡 플레이(Coupang Play), 토스(Toss), Tableau, Redash, GPU Rendering, Legacy Bug, 채용 전략

## 3교시 11:00 - 12:00

#### 📌 강의 핵심 요약
효과적인 포트폴리오 구성을 위한 주피터 노트북 활용법과 데이터 엔지니어링 프로젝트의 단계별 목표 설정에 대해 다룹니다. 단순한 코드 나열이 아닌 서사(Narrative)가 있는 결과물의 중요성을 강조하며, 팀별로 달성 가능한 목표 범위를 설정하고 엔드투엔드(End-to-End) 파이프라인 구축을 경험해 봅니다.

#### 📚 상세 내용 정리

##### 1. 효과적인 포트폴리오 구성: 주피터 노트북(Jupyter Notebook)
- **개념 설명**: 강사는 PPT 형식보다 주피터 노트북과 같은 문서 형태를 선호합니다. 데이터 임포트부터 분석, 시각화 과정이 위에서 아래로 흐르는 '내러티브'를 보여주기 때문입니다.
- **예시/실습 내용**:
  - 코드뿐만 아니라 주석(Comment)을 통해 각 로직을 정확히 이해하고 작성했음을 증명해야 합니다.
  - 주피터 노트북 내에서 분석 인사이트를 함께 정리하여 그 자체로 완성된 리포트가 되도록 구성합니다.
- **주의사항 또는 팁**: AI의 도움을 받더라도 각 코드 라인의 의미를 스스로 설명할 수 있어야 실전 면접에서 대응 가능합니다.

##### 2. 데이터 프로젝트의 범위와 목표 설정
- **개념 설명**: 본 과정은 '분석 엔지니어링(Analytics Engineering)'에 초점을 맞춥니다. 수집(Collection)에서 인사이트 도출까지의 전체 파이프라인을 경험하는 것이 핵심입니다.
- **예시/실습 내용**:
  - **프로젝트 단계**: 1단계(수집/적재) -> 2단계(처리/변환) -> 3단계(대시보드/인사이트).
  - 실시간 주식/코인 데이터 모니터링처럼 데이터 수집 단계만으로도 훌륭한 엔지니어링 프로젝트가 될 수 있습니다.
- **주의사항 또는 팁**:
  - 3일이라는 한정된 시간 내에 모든 단계를 완벽히 하기 어려울 수 있습니다. 팀별로 실현 가능한 목표(Scope)를 정하고, 발표 시 본인들이 집중한 단계를 명확히 설명하는 것이 중요합니다.

#### 💡 핵심 포인트
- "포트폴리오는 상대방이 내 사고의 흐름을 읽을 수 있도록 내러티브를 갖춰야 합니다."
- "주피터 노트북은 코드 실력과 분석 능력을 동시에 보여줄 수 있는 최고의 도구입니다."
- "목표를 너무 높게 잡기보다, 정해진 시간 내에 한 단계라도 제대로 완성하는 것이 더 의미 있습니다."

#### ✅ 실습/과제
- 금요일 프로젝트 발표를 위한 팀별 주제 선정 및 범위(Scope) 정의하기.
- 팀원 간 역할 분담 및 데이터 수집원 확보.

#### 🔗 관련 키워드
- Jupyter Notebook, Portfolio Narrative, Analytics Engineering, Data Pipeline, Scope Management, Team Project

## 🍽️ 점심시간 12:00 - 13:00

## 4교시 13:00 - 14:00

#### 📌 강의 핵심 요약
GitHub Actions를 활용한 자동 배포(CI/CD) 파이프라인 구축 실습을 진행합니다. YAML 파일을 통해 배포 워크플로우를 정의하고, Git Push 시 자동으로 Airflow VM에 DAG 파일이 배포되는 과정을 학습합니다. 트리거 설정, SSH 배포 단계, 그리고 실시간 반영까지의 전체 흐름을 이해합니다.

#### 📚 상세 내용 정리

##### 1. GitHub Actions 워크플로우 이해
- **개념 설명**: GitHub Actions는 YAML 파일로 정의된 자동화 워크플로우입니다. 코드 변경 시 자동으로 빌드, 테스트, 배포를 수행할 수 있습니다.
- **예시/실습 내용**:
  - **YAML 파일 구조**: 트리거(on push), 작업(jobs), 단계(steps)로 구성
  - **배포 프로세스**:
    1. 코드 체크아웃 (Checkout code)
    2. SSH를 통한 VM 접속
    3. 파일 전송 및 권한 설정
    4. Airflow에 DAG 반영 (약 3-5분 소요)
- **주의사항 또는 팁**:
  - Push 후 GitHub Actions 탭에서 실행 상태를 실시간으로 확인할 수 있습니다.
  - 배포 완료 후 Airflow UI에 반영되기까지 시간이 다소 걸릴 수 있습니다.

##### 2. DAG 파일 네이밍 규칙 및 배포 실습
- **개념 설명**: 팀 프로젝트에서는 파일 네이밍 규칙을 통일하여 충돌을 방지해야 합니다.
- **예시/실습 내용**:
  - **네이밍 규칙**: `popcorn_[본인이름]_[dag_id].py` 형태로 작성
  - **실습 단계**:
    1. 샘플 DAG 파일 복사
    2. 본인 폴더에 붙여넣기
    3. DAG ID 및 오너 정보 수정
    4. Git Commit & Push
    5. GitHub Actions 자동 실행 확인
- **주의사항 또는 팁**:
  - 타인의 폴더/파일은 절대 수정하지 않습니다.
  - Airflow의 `owner` 값과 GitHub 계정을 일치시켜 실수 방지 로직을 추가할 수 있습니다.

#### 💡 핵심 포인트
- "YAML 파일은 설계도입니다. 순서와 방법을 명확히 정의하면 자동화가 가능합니다."
- "GitHub Actions는 내 코드를 대신 검증하고 배포해주는 도구입니다."
- "팀 프로젝트에서는 네이밍 규칙과 권한 관리가 매우 중요합니다."

#### ✅ 실습/과제
- GitHub Actions를 통한 DAG 자동 배포 성공시키기.
- Airflow UI에서 본인의 DAG가 정상적으로 등록되었는지 확인하기.

#### 🔗 관련 키워드
- GitHub Actions, YAML, CI/CD, SSH Deploy, Airflow DAG, Git Push, Naming Convention, Ownership


## 5교시 14:00 - 15:00

#### 📌 강의 핵심 요약
GitHub Actions 배포 과정에서 발생하는 오류를 디버깅하고 해결하는 실습을 진행합니다. AI 도구를 활용하여 Git 작업(commit, push)을 자동화하며, 배포가 정상적으로 완료되어 Airflow에 반영되기까지의 전체 프로세스를 반복 학습합니다.

#### 📚 상세 내용 정리

##### 1. 배포 오류 디버깅 및 해결
- **개념 설명**: 실무에서는 배포 과정에서 다양한 오류가 발생할 수 있으며, 로그를 통해 원인을 파악하고 해결해야 합니다.
- **예시/실습 내용**:
  - GitHub Actions 로그에서 오류 메시지 확인
  - SSH 연결 문제, 파일 권한 문제 등 트러블슈팅
  - 오류 수정 후 재배포 및 검증
- **주의사항 또는 팁**:
  - 오류가 발생하면 당황하지 말고 로그를 차근차근 읽어봅니다.
  - AI 도구를 활용하여 해결 방법을 빠르게 찾을 수 있습니다.

##### 2. AI를 활용한 Git 작업 자동화
- **개념 설명**: 반복적인 Git 작업(add, commit, push)을 AI 도구에게 요청하여 효율성을 높일 수 있습니다.
- **예시/실습 내용**:
  - AI에게 "커밋하고 푸시해줘" 명령
  - 커밋 메시지 자동 생성 및 실행
  - GitHub Actions 자동 트리거 확인
- **주의사항 또는 팁**:
  - AI가 자동으로 작업하더라도 결과는 반드시 직접 확인해야 합니다.
  - 배포 반영까지 3-5분 정도 소요될 수 있습니다.

#### 💡 핵심 포인트
- "오류는 학습의 기회입니다. 로그를 읽고 해결하는 과정이 실력 향상의 핵심입니다."
- "AI는 반복 작업을 줄여주는 도구이지만, 최종 검증은 개발자의 몫입니다."
- "배포 자동화를 익히면 실무에서 큰 생산성 향상을 경험할 수 있습니다."

#### ✅ 실습/과제
- 배포 오류 발생 시 스스로 디버깅하고 해결해보기.
- AI를 활용한 Git 작업 자동화 경험해보기.

#### 🔗 관련 키워드
- Debugging, Error Log, SSH Connection, AI Automation, Git Commit, Git Push, Deployment Troubleshooting


## 6교시 15:00 - 16:00

#### 📌 강의 핵심 요약
Supabase 데이터베이스 연결 설정을 실습합니다. Airflow Connection을 통해 Supabase PostgreSQL에 안전하게 접속하는 방법을 학습하며, 환경 변수 관리와 보안의 중요성을 이해합니다.

#### 📚 상세 내용 정리

##### 1. Supabase 접속 정보 설정
- **개념 설명**: Supabase는 PostgreSQL 기반의 클라우드 데이터베이스 서비스입니다. Airflow에서 안전하게 접속하기 위해 Connection 설정이 필요합니다.
- **예시/실습 내용**:
  - **Supabase Connection 정보**:
    - Connection ID: `supabase_conn`
    - Connection Type: `Postgres`
    - Host, Port, Database, User, Password 입력
  - **Transaction Pooler 모드 사용** (Direct 모드 아님)
  - Airflow Admin > Connections에서 설정
- **주의사항 또는 팁**:
  - 비밀번호와 같은 민감 정보는 절대 코드에 하드코딩하지 않습니다.
  - Airflow Connection을 통해 중앙화된 관리가 가능합니다.

##### 2. 데이터 엔지니어링의 복잡성 이해
- **개념 설명**: 데이터 엔지니어링은 설정이 많고 복잡하지만, 한 번 제대로 익히면 강력한 무기가 됩니다.
- **예시/실습 내용**:
  - 오늘 하루 동안 수행한 작업들 (Airflow 설치, GitHub Actions 설정, Supabase 연결)은 실무에서 보통 3-4명의 엔지니어가 몇 주간 작업하는 분량
  - AI의 도움으로 빠르게 학습하고 구현 가능
- **주의사항 또는 팁**:
  - 설정이 어렵더라도 포기하지 말고 AI에게 질문하며 학습합니다.
  - 실무에서는 이러한 설정 작업을 할 줄 아는 것만으로도 큰 가치가 있습니다.

#### 💡 핵심 포인트
- "Supabase Connection 설정은 프로덕션 환경에서 매우 중요한 보안 요소입니다."
- "오늘 하루 배운 내용은 실무 3-4주 분량의 작업입니다. 스스로를 칭찬하세요."
- "AI 시대에는 설정 방법을 외우는 것보다, 어떻게 구성되는지 이해하는 것이 중요합니다."

#### ✅ 실습/과제
- Supabase Connection을 Airflow에 등록하기.
- 샘플 DAG를 실행하여 Supabase에 테스트 데이터가 적재되는지 확인하기.

#### 🔗 관련 키워드
- Supabase, PostgreSQL, Airflow Connection, Transaction Pooler, Environment Variable, Security, Database Connection


## 7교시 16:00 - 17:00

#### 📌 강의 핵심 요약
Airflow를 통한 Supabase 데이터 적재 실습을 완료하고, 전체 파이프라인이 정상 작동하는지 검증합니다. 오늘 배운 내용을 복습하며, 데이터 엔지니어로서의 첫걸음을 축하합니다.

#### 📚 상세 내용 정리

##### 1. Supabase 데이터 적재 테스트
- **개념 설명**: Airflow DAG를 실행하여 Supabase에 실제 데이터가 적재되는지 확인하는 End-to-End 테스트를 수행합니다.
- **예시/실습 내용**:
  - 샘플 DAG (13번) 복사 및 수정
  - DAG 실행 후 Supabase 테이블 생성 확인
  - `airflow_test` 테이블에 데이터가 정상적으로 삽입되었는지 검증
- **주의사항 또는 팁**:
  - 본인의 Supabase DB에만 접근해야 하며, 타인의 DB에 접근하면 안 됩니다.
  - Connection 정보가 잘못되면 강사의 DB에 데이터가 쌓일 수 있으니 주의합니다.

##### 2. 오늘 배운 내용 총정리
- **개념 설명**: 오늘 하루 동안 Airflow 설치부터 GitHub Actions, Supabase 연결까지 데이터 엔지니어링의 핵심 인프라를 모두 구축했습니다.
- **예시/실습 내용**:
  - **오늘의 성과**:
    - VM에 Airflow 설치 및 실행
    - GitHub Actions를 통한 CI/CD 파이프라인 구축
    - Supabase 데이터베이스 연결 및 데이터 적재 성공
  - 이는 실무에서 시니어 엔지니어 3-4명이 몇 주간 작업하는 분량
- **주의사항 또는 팁**:
  - 오늘 배운 내용을 반드시 복습하고, 막히는 부분은 적극적으로 질문합니다.
  - 내일은 실제 지하철 데이터를 수집하여 Supabase에 적재하는 실습을 진행할 예정입니다.

#### 💡 핵심 포인트
- "여러분은 오늘 하루 만에 실무 데이터 엔지니어링 파이프라인을 구축했습니다."
- "Airflow를 다룰 줄 안다는 것만으로도 실무에서 큰 경쟁력이 됩니다."
- "AI의 도움을 받았지만, 여러분이 직접 이해하고 실행했다는 것이 중요합니다."
- "스스로에게 축하와 칭찬을 아끼지 마세요. 대단한 성과입니다."

#### ✅ 실습/과제
- Supabase에 테스트 데이터가 정상적으로 적재되었는지 최종 확인.
- 오늘 배운 GitHub Actions, Airflow, Supabase 연동 과정 복습.
- 궁금한 점이나 오류가 있다면 강사에게 질문하기.

#### 🔗 관련 키워드
- End-to-End Test, Data Pipeline, Supabase Table, Database Insert, Data Engineering, Airflow Success, Career Development


## 8교시 17:00 - 18:00