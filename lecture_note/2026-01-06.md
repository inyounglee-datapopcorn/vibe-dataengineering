# 📝 2026-01-06 

---
[Tina Huang New Video](https://youtu.be/JHF2t-S2nm0?si=OGppG_KpbfR2HIz_)


## 1교시 - 09:00 ~ 12:00
> **주제:** AI 도구(Antigravity) 심화 활용 및 Git/GitHub 협업 기초
---
### 📌 오늘의 강의 핵심 요약
이번 강의에서는 **Google Antigravity**의 에이전트(Agent), 아티팩트(Artifacts), 멀티모달 등 심화 기능을 마스터하고, AI 사용 시 필수적인 **'Human-in-the-loop(인간 개입)'**의 중요성을 다루었습니다. 후반부에는 협업의 필수 도구인 **Git & GitHub**의 개념을 이해하고 첫 저장소를 구축하는 실습을 진행했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ Google Antigravity: 스마트하고 안전한 AI 활용법

![Antigravity Features Infographic](./asset/image/antigravity_infographic.png)
*   **시스템 구성**: VS Code를 기반으로 제작되어 익숙한 사용 환경을 제공합니다.
*   **4대 핵심 구성**:
    *   **Agent**: 사용자의 명령을 직접 수행하는 인공지능 주체
    *   **Editor**: 실시간 코드 작성 및 수정이 이루어지는 공간
    *   **Artifacts**: AI의 작업 계획(Plan)과 중간 결과물을 시각화하는 보고서
    *   **Agent Manager**: 여러 에이전트를 동시에 제어하는 클라우드 관제탑
*   **안전한 AI 제어 (Human-in-the-loop)**:
    *   **Request Review**: AI가 파일 삭제 등 민감한 작업을 하기 전 사용자의 승인을 받도록 설정
    *   **Prompting 전략**:
        *   `Soft`: AI의 자율성을 존중하는 권장 사항
        *   `Strict`: 절대 어겨서는 안 되는 강제 지침

#### 2️⃣ AI의 멀티모달 & 파일 시스템 제어
*   **멀티모달(Multimodal)**: 텍스트를 넘어 이미지 분석 및 생성까지 가능 (예: 사진 속 환경 분석 등)
*   **강력한 파일 제어**: AI는 코드 생성뿐만 아니라 파일 삭제(`Delete`) 권한도 가집니다.
*   ⚠️ **주의 사항**: `.env`와 같은 보안 파일이나 중요한 DB 파일이 AI에 의해 실수로 노출/삭제되지 않도록 철저한 검토가 필요합니다.

#### 3️⃣ Git & GitHub: 버전 관리와 협업의 시작
*   **Git의 정의**: 파일의 변경 이력을 추적하고 관리하는 분산 버전 관리 시스템
*   **핵심 가치**: '누가, 언제, 무엇을' 수정했는지 기록하여 과거 시점으로 복구하거나 협업 충돌을 방지함
*   **도구 선택**:
    *   **CLI (터미널)**: `git add`, `git commit` 등 명령어로 정밀 제어
    *   **GUI (그래픽)**: GitHub Desktop 등을 통해 마우스 클릭으로 직관적 관리
*   ✨ **Tip**: 최근 AI는 변경된 내용을 분석해 **Commit Message**를 자동으로 생성해 주기도 합니다.

#### 4️⃣ 실무 Git 명령어 흐름 (CLI Reference)
실제 터미널에서 협업 시 사용하는 표준 프로세스입니다.
1.  `git status`: 현재 변경된 파일 상태 확인
2.  `git add .`: 변경 사항을 기록 대기 상태(Staging Area)로 추가
3.  `git commit -m "메시지"`: 현재 상태를 하나의 버전으로 확정
4.  `git push`: 확정된 버전을 원격 저장소(GitHub)로 전송

---

### 💡 핵심 포인트 (Takeaway)
*   **AI의 통제권**: AI는 도구일 뿐, 최종 의사결정은 인간이 내리는 **'Review 습관'**이 가장 중요합니다.
*   **보안 제일**: AI 에이전트가 접근할 수 있는 범위와 보안 파일 보호에 대해 항상 인지해야 합니다.
*   **협업의 기술**: Git 사용 시 가급적 작업 영역을 나누어 **충돌(Conflict)**을 최소화하는 것이 초기 학습 단계에서 유리합니다.

---

### ✅ 오늘의 실습 과제
**GitHub 첫 저장소(Repository) 구축하기**
1.  GitHub 로그인 후 상단 `+` 버튼 → `New repository` 클릭
2.  이름 설정: **`KDT_Woongjin`** 입력
3.  초기화 옵션: **`Add a README file`** 체크 (필수)
4.  완료: **`Create repository`** 버튼 클릭

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Google Antigravity** | AI 기반의 지능형 코드 에디터 |
| **Human-in-the-loop** | AI의 판단에 인간이 개입하여 안전성을 높이는 방식 |
| **Commit (커밋)** | 변경된 사항을 하나의 '버전'으로 저장소에 남기는 행위 |
| **Push / Pull** | 로컬 버전을 올리거나(Push), 원격 버전을 가져오는(Pull) 작업 |
| **Conflict (충돌)** | 동일한 파일의 같은 부분을 여러 명이 수정했을 때 발생하는 현상 |


## 2교시 - 10:00 ~ 11:00
> **주제:** GitHub Desktop 실무 활용 및 AI 기반 코드 복구(Rollback) 실습
---
### 📌 오늘의 강의 핵심 요약
이론으로 배운 Git을 **GitHub Desktop**을 통해 실제 환경에서 다루는 **Hands-on** 실습을 진행했습니다. 저장소 복제(Clone)부터 수정 사항 반영(Push)까지의 전체 워크플로우를 익히고, **Commit ID**를 활용하여 AI와 함께 안전하게 이전 상태로 되돌리는(Rollback) 고급 기술과 협업을 위한 **Branch/Fork** 개념을 학습했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ GitHub Desktop 설치 및 연동
* **설정 및 보안**: GitHub Desktop 설치 후 계정 연동 및 **2단계 인증(MFA)** 설정 권장
* **저장소 복제 (Clone)**: GitHub 원격 저장소를 로컬 컴퓨터로 복사하여 작업 환경 구축
* **에디터 연결**: GitHub Desktop에서 `Open in external editor` 또는 `Open Folder`를 통해 Antigravity와 연동

#### 2️⃣ Git 표준 워크플로우 실습
* **파일 수정 및 저장**: 에디터에서 작업을 수행하고 반드시 `Ctrl+S`로 저장 (파일명 옆의 하얀 점 확인)
* **변경 사항 확인**: GitHub Desktop의 `Changes` 탭에서 시각적으로 수정 내용 검토
* **커밋 (Commit)**: 변경 내용을 요약하는 제목(Summary)을 작성하여 로컬 버전에 기록
* **푸시 (Push)**: 로컬의 변경 사항을 원격 저장소(GitHub)로 최종 업로드

#### 3️⃣ 고급 기능: AI를 활용한 롤백(Rollback)
* **롤백의 필요성**: 잘못된 코드 반영 시 안전하게 이전 시점으로 복구가 필요한 경우
* **복구 프로세스**:
    1. `History` 탭에서 복구 시점의 **Commit ID (SHA)** 복사
    2. AI에게 **"이 ID 시점으로 롤백해줘(Reset)"**라고 구체적으로 요청
    3. AI의 안내에 따라 `git reset --hard` 등의 명령어 수행 및 강제 푸시(Force Push) 검토
* **주의사항**: 롤백은 작업 내역이 유실될 수 있으므로 상황을 AI에게 명확히 설명 후 진행 권장

#### 4️⃣ 협업을 위한 데이터 관리 (Branch & Fork)
* **브랜치 (Branch)**: 메인 코드(Main)를 보호하면서 안전하게 기능을 추가하기 위한 독립적인 작업 줄기
* **포크 (Fork)**: 타인의 저장소 전체를 내 계정으로 복사하여 자유롭게 실험할 수 있는 기능

---

### 💡 핵심 포인트 (Takeaway)
* **직관적인 도구 활용**: CLI가 익숙하지 않다면 GUI 툴(GitHub Desktop)을 사용하여 실수를 방지하세요.
* **AI 해결사**: 복합적인 Git 문제 발생 시 현재 상태와 목표를 AI에게 설명하여 해결책을 도출하세요.
* **저장 습관**: 에디터에서 파일이 저장되지 않은 상태(흰 점)에서는 Git이 변경 사항을 인식할 수 없습니다.

---

### ✅ 실습 과제
**GitHub Desktop을 활용한 자기소개 업데이트**
1. **Clone**: 본인의 `KDT_Woongjin` 저장소를 로컬로 복제합니다.
2. **Modify**: `README.md`에 자기소개를 추가하고 파일을 저장합니다.
3. **Commit & Push**: 의미 있는 커밋 메시지를 작성하고 GitHub에 반영합니다.
4. **Verify**: 웹 브라우저에서 최종 반영 여부를 확인합니다.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Clone** | 원격 저장소를 로컬로 복제하는 작업 |
| **Commit ID (SHA)** | 각 커밋에 부여되는 고유한 식별 번호 |
| **Rollback / Reset** | 이전 버전 상태로 되돌리는 작업 |
| **Force Push** | 원격 저장소의 내용을 강제로 덮어쓰는 작업 (주의 필요) |
| **MFA** | 계정 보안 강화를 위한 다중 요소 인증 |

---

## 3교시 - 11:00 ~ 11:30
> **주제:** Markdown 생산성 팁 및 데이터 엔지니어링 파이프라인 로드맵
---
### 📌 오늘의 강의 핵심 요약
문서화 도구인 **Markdown**의 고급 활용법과 에디터 생산성을 높여주는 **단축키**를 실습했습니다. 이어지는 오후 실습을 위해 공공 데이터 수집부터 적재까지의 **데이터 파이프라인** 전체 공정을 조망하고, **Supabase, DBeaver, SQLAlchemy** 등 주요 기술 스택의 역할을 이해했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ Markdown & 에디터 마스터링
* **Markdown 줄바꿈**: 문장 끝에 **스페이스 두 번** 입력 또는 전용 태그 활용
* **이미지 및 링크**: `![Alt](URL)` 포맷을 사용하며, AI 에이전트에게 특정 폴더로 이미지 저장을 명령할 수도 있음
* **Antigravity 필수 단축키**:
    * **파일 검색**: `Ctrl + P` (Mac: `Cmd + P`)를 통한 빠른 이동
    * **분할 미리보기**: `Ctrl + K` 후 `V`
    * **전체 미리보기**: `Ctrl + Shift + V`

#### 2️⃣ 데이터 파이프라인 구축 로드맵 (Data Flow)
* **목표**: 공공 데이터 API(지하철/버스 등) 데이터를 수집하여 클라우드 DB에 적재
* **기술 스택 (Tech Stack)**:
    * **Language**: Python 3.10+
    * **Library**: `Pandas`(가공), `Requests`(수집), **`SQLAlchemy`**(ORM - 객체 기반 DB 조작)
    * **Database**: **Supabase** (PostgreSQL 기반의 클라우드 BaaS)
    * **Tool**: **DBeaver** (통합 데이터베이스 관리 도구)

#### 3️⃣ 데이터 엔지니어의 역할: Metric Store
* **지표의 통일**: 부서별로 다르게 정의될 수 있는 지표(예: 매출 기준)를 전사적으로 하나로 관리
* **자동화 (CI/CD)**: **GitHub Actions**를 통한 이벤트 감지 및 **Airflow**를 활용한 워크플로우 자동화

---

### 💡 핵심 포인트 (Takeaway)
* **도구 숙련도**: 단축키 하나가 업무 효율을 좌우합니다. 기본적인 단축키는 반드시 체득하세요.
* **AI 지시 원칙**: AI에게 결과물을 맡길 때는 구체적인 위치(폴더 경로 등)를 지정하고 최종 검토를 거쳐야 합니다.
* **기록의 가치**: 학습 노트를 GitHub에 지속적으로 관리하여 나만의 기술 포트폴리오로 만드세요.

---

### ✅ 실습 과제
1. **Markdown 실습**: 본인 노트에 외부 링크와 이미지를 삽입하고 줄바꿈 규칙을 적용해 보세요.
2. **에디터 숙달**: `Ctrl+P`와 미리보기 단축키를 활용해 파일 사이를 빠르게 오가 보세요.
3. **환경 준비**: 오후 실습을 위해 **Supabase** 계정 생성 및 로그인을 완료합니다.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **SQLAlchemy** | 파이썬 코드로 SQL을 작성하는 ORM 툴킷 |
| **Supabase** | 오픈소스 Firebase 대안인 클라우드 DB 서비스 |
| **DBeaver** | 다양한 DB를 관리할 수 있는 GUI 클라이언트 프로그램 |
| **Metric Store** | 데이터 지표의 정의를 관리하고 통일하는 시스템 |
| **Data Pipeline** | 데이터의 수집, 가공, 적재가 이루어지는 일련의 과정 |

---

## 4교시 - 13:00 ~ 14:00
> **주제:** 공공 데이터 API 활용 및 인증키(Authentication Key) 발급 실습
---
### 📌 오늘의 강의 핵심 요약
실제 데이터 수집을 위해 **공공데이터포털**과 **서울 열린데이터광장**의 API 사용법을 익혔습니다. 실시간 지하철 정보 등 제공되는 데이터의 규격(JSON/XML)을 이해하고, 보안이 필수적인 **API 인증키**를 안전하게 발급받아 테스트하는 과정을 수행했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ 공공 데이터 소스 탐색
* **공공데이터포털 (data.go.kr)**: 행정안전부 관리, 상권 정보/날씨 등 국가 표준 데이터 제공
* **서울 열린데이터광장 (data.seoul.go.kr)**: 서울시 특화 데이터(지하철/버스 실시간 정보) 제공
* **지하철 실시간 API**:
    * **위치 정보**: 현재 열차의 상태(진입, 도착, 출발)를 코드로 제공
    * **도착 정보**: 도착 예정 시간(`barvlDt`) 등의 세부 지표 포함

#### 2️⃣ API 인증키(Key) 발급 프로세스
1. **회원가입 및 데이터 신청**: 포털 내에서 원하는 데이터셋을 찾아 활용 신청 수행
2. **활성화 대기**: 신청 즉시 사용이 불가한 경우가 많으며, 서버 동기화에 약 **1시간 내외** 소요됨
3. **브라우저 테스트**: 발급받은 키를 URL 파라미터에 포함하여 JSON/XML 데이터가 정상 출력되는지 확인

#### 3️⃣ API 보안 및 데이터 해석 전략
* **데이터 해석**: 난해한 필드명이나 코드 값은 명세서와 함께 AI에게 전달하여 **'AI 해석기'**로 활용
* **보안 관리**: API 키는 비밀번호와 같이 취급해야 하며, 절대로 공개 저장소(GitHub)에 직접 노출하지 않음
* **키 관리 팁**: `1Password` 또는 슬랙의 개인 메시지 창 등을 활용하여 파라미터와 함께 기록 관리

---

### 💡 핵심 포인트 (Takeaway)
* **데이터 활용 역량**: 공공 데이터를 활용하여 시장 분석이나 개인 프로젝트의 퀄리티를 높일 수 있습니다.
* **유연한 대처**: API 키 미작동이나 데이터 오류 발생 시 AI를 적극 활용하여 에러 메시지를 분석하세요.
* **보안 의식**: 환경 변수를 활용한 API 키 보호 습관은 데이터 엔지니어링의 기본입니다.

---

### ✅ 실습 과제
**서울시 지하철 API 연동 테스트**
1. **인증키 신청**: 서울 열린데이터광장에서 '실시간 지하철 도착 정보' API 키를 신청합니다.
2. **테스트 수행**: 샘플 주소에 본인의 인증키를 입력하여 브라우저에서 데이터가 호출되는지 확인합니다.
3. **데이터 검토**: 출력된 JSON/XML 데이터 중 필요한 필드(`barvlDt`, `trainLineNm` 등)를 식별해 봅니다.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Open API** | 누구나 접근 가능한 공개형 인터페이스 |
| **Authentication Key** | API 서비스 이용을 위한 고유의 보안 인증값 |
| **JSON / XML** | 컴퓨터 간 데이터를 주고받기 위한 표준 데이터 포맷 |
| **Environment Variable** | 보안이 필요한 키 등을 소스코드 외부에서 관리하는 설정값 |


## 5교시 - 14:00 ~ 15:00
> **주제:** API 통신 원리 이해 및 실시간 지하철 정보 데이터 분석
---
### 📌 오늘의 강의 핵심 요약
발급받은 인증키를 활용해 **실시간 지하철 위치 및 도착 정보 API**를 직접 호출하고, 응답 데이터(XML/JSON)의 구조를 정밀 분석했습니다. API 문서와 실제 동작 간의 차이를 발견하는 과정을 통해 데이터 정합성의 중요성을 익히고, **CURL**과 **JQ**를 활용한 터미널 환경에서의 API 핸들링 능력을 배양했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ API 응답 데이터 포맷의 이해
* **XML vs JSON**:
    * **XML**: 태그 기반 구조로 가독성이 좋으나 데이터량이 상대적으로 큼
    * **JSON**: Key-Value 쌍으로 구성되어 데이터 구조 파악이 용이하며 경량화된 포맷
* **HTTP 상태 코드 (Status Code)**:
    * `200`: 성공적인 요청 처리
    * `404`: 요청한 리소스를 찾을 수 없음 (URL 등 확인 필요)
    * `500`: 서버 내부 오류 발생

#### 2️⃣ 오픈소스 기여와 데이터 정합성 (Contribution)
* **오류 발견**: API 문서에는 인덱스가 `0`부터 시작한다고 명시되어 있으나, 실제 호출 시 `1`부터 조회되는 현상 확인
* **기여의 가치**: 버그나 문서 오류 발견 시 **Issue**를 제기하거나 **Pull Request (PR)**를 보내는 활동은 본인의 실무 역량을 증명하는 훌륭한 포트폴리오가 됨

#### 3️⃣ 터미널 환경의 API 호출 실습
* **CURL**: 커맨드 라인에서 서버와 데이터를 전송하는 표준 도구
* **JQ**: 터미널에서 JSON 데이터를 보기 좋게 출력(Pretty Print)하고 필터링하는 도구
* **팁**: 한글 깨짐이나 OS별 명령어 차이는 AI 가이드를 활용하여 인코딩 설정 등으로 해결 가능

#### 4️⃣ API의 개념적 이해 (식당 비유)
* **Client (사용자)**: 메뉴(데이터)를 주문하는 손님
* **Server (데이터베이스)**: 요리(처리)를 수행하는 주방
* **API (웨이터)**: 손님의 주문을 주방에 전달하고 완성된 요리를 가져다주는 **중개자**
* **Auth Key (인증키)**: 허가된 손님만 입장할 수 있는 출입증

---

### 💡 핵심 포인트 (Takeaway)
* **데이터 타입 엄수**: JSON 응답에서 숫자가 따옴표(`" "`)로 감싸져 있다면 문자열이므로, 연산 시 형 변환 작업이 필수입니다.
* **보안 의식**: HTTP 요청은 데이터가 평문으로 전송되므로 API 키 탈취 위험이 있습니다. 가급적 HTTPS 사용을 권장합니다.
* **문제 해결력**: API 문서와 실제 데이터가 다를 때 당황하지 않고 직접 테스트하여 원인을 파악하는 습관을 기르세요.

---

### ✅ 실습 과제
1. **터미널 통신**: 제공된 CURL 명령어를 터미널에 입력하여 실제 지하철 데이터를 조회해 보세요.
2. **에러 핸들링**: 필수 파라미터를 누락시켜 보고 서버가 반환하는 에러 메시지를 확인합니다.
3. **타입 확인**: 응답 데이터 중 `1001` 같은 코드가 문자열인지 숫자인지 직접 확인해 보세요.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **CURL / JQ** | 터미널 기반의 API 호출 및 데이터 처리 도구 |
| **Status Code** | 요청에 대한 서버의 응답 상태를 나타내는 숫자 코드 |
| **PR (Pull Request)** | 소스코드나 문서의 수정 사항을 제안하여 반영을 요청함 |
| **Data Integrity** | 데이터가 전 과정에서 일관되고 정확하게 유지되는 상태 |



## 6교시 - 15:00 ~ 16:00
> **주제:** OpenAI API 실습 및 Supabase 데이터베이스 설계 기초
---
### 📌 오늘의 강의 핵심 요약
**REST API**의 핵심 메서드인 **GET**과 **POST**의 차이점을 명확히 이해하고, **OpenAI API**를 직접 호출하여 **토큰(Token)** 단위의 과금 체계와 모델 구동 원리를 확인했습니다. 또한, **Supabase**를 활용하여 데이터베이스 테이블을 구축하고, 글로벌 서비스 운영의 필수 요소인 **타임존(Timezone)** 처리 기법을 실습했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ REST API 심화: GET vs POST
* **GET (조회)**: 서버의 데이터를 단순히 가져올 때 사용 (예: 메뉴판 열람)
* **POST (생성/전송)**: 서버에 데이터를 보내거나 새로운 리소스를 생성할 때 사용 (예: 음식 주문, 로그인 정보 전송)
* **API 구분**: 누구나 사용 가능한 **Open API**와 특정 권한이 필요한 **비공개 API**의 차이점 인지

#### 2️⃣ OpenAI API와 토큰(Token) 과금 체계
* **호출 구조**: 헤더(`Authentication`, `Content-Type`)와 바디(`model`, `messages`)를 포함한 POST 요청
* **토큰 이해**: AI가 텍스트를 인식하는 최소 단위
    * 한글은 영어보다 많은 토큰을 소모하는 경향이 있음 (글자 수의 약 1.5~2배)
    * **과금 방식**: 100만 토큰당 비용이 산정되며, 고성능 모델일수록 비용이 기하급수적으로 상승
* **엔지니어 마인드**: 기능 구현뿐만 아니라 **비용 효율성**을 고려한 모델 선택이 중요

#### 3️⃣ Supabase를 활용한 DB 및 타임존 설정
* **테이블 설계**: `student` 테이블 생성 및 적절한 데이터 타입(`Text`, `Integer 2`, `Timestamptz`) 지정
* **타임존(Timezone) 이슈**: 기본 설정인 **UTC**(협정 세계시)를 한국 시간(**KST**, UTC+9)으로 조정하는 실습 수행
* **해결책**: 컬럼 기본값을 `now() at time zone 'Asia/Seoul'`로 설정하여 데이터 생성 시점 자동 보정

---

### 💡 핵심 포인트 (Takeaway)
* **비용 최적화**: API 사용 전 예상 토큰 소모량과 비용을 미리 계산하는 습관을 들이세요.
* **타입의 경제성**: 데이터 성격에 맞는 최소 크기의 타입(Integer 2 vs 8)을 선택하여 저장 효율을 높이세요.
* **공식 문서 활용**: OpenAI, Supabase 등 방대한 서비스의 최신 정보는 항상 공식 레퍼런스에서 확인하는 것이 가장 정확합니다.

---

### ✅ 실습 과제
1. **OpenAI 호출**: CURL 명령어로 질문을 던져보고, 응답 데이터에서 사용된 `usage` (토큰 수)를 확인합니다.
2. **DB 타임존 수정**: `student` 테이블의 `created_at` 설정을 변경하여 데이터 입력 시 한국 시간이 반영되는지 검증합니다.
3. **컬럼 추가**: GUI 환경에서 `gender` 등의 새로운 컬럼을 추가하며 DB 스키마 변경 프로세스를 익힙니다.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **REST API** | HTTP를 기반으로 하는 웹 서비스 설계 아키텍처 |
| **Token (토큰)** | LLM 서비스의 기본 처리 및 과금 단위 |
| **UTC / KST** | 세계 표준 시간대와 한국 표준 시간대의 차이와 변환 |
| **Timestamptz** | 시간대 정보를 포함하는 날짜/시간 데이터 타입 |
| **PostgreSQL** | Supabase의 기반이 되는 강력한 오픈소스 관계형 DB |


## 7교시 - 16:00 ~ 17:00
> **주제:** 데이터 리터러시(Data Literacy) 및 SQL 실무 기초
---
### 📌 오늘의 강의 핵심 요약
데이터 엔지니어링의 본질인 **"데이터의 원천과 의미"**를 파악하는 **데이터 리터러시**의 중요성을 학습했습니다. **Supabase SQL Editor**를 통해 기본적인 조회 쿼리를 실습하고, 글로벌 서비스의 핵심인 **Timezone** 체계를 이해했습니다. 마지막으로 비즈니스 가설을 세우고 논리적으로 추론하는 사고 실험을 진행하며 프로젝트 기획 역량을 다졌습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ 데이터 엔지니어의 마인드셋: Why에 집중하기
* **데이터 원천(Origin) 의심**: 단순히 제공된 데이터를 넘어, 어떤 비즈니스 모델(BM)에서 생성된 데이터인지 파악하는 습관 필요
* **데이터 리터러시(Literacy)**: 코드 작성 능력보다 중요한 것은 데이터의 행간을 읽고 **비즈니스 인사이트**를 도출하는 능력
* **기획의 중요성**: 포트폴리오 구성 시 **'왜(Why)'** 이 프로젝트를 기획했는지에 대한 논리가 기술적 구현보다 우선되어야 함

#### 2️⃣ Supabase SQL Editor 실습
* **조회 쿼리 수행**: SQL Editor에서 `SELECT * FROM public.student`를 실행하여 적재된 데이터 확인
* **유연한 대응력**: 현업에서는 모든 문법을 완벽히 외우기보다, 에러 메시지를 분석하며 데이터 타입(Integer 2 -> 4) 등을 수정하는 유연한 접근이 효율적일 수 있음

#### 3️⃣ 글로벌 표준: Timezone (시간대) 이해
* **UTC (협정 세계시)**: 영국 기준의 표준 시(0시)로, 모든 시계의 기준점
* **한국 표준시 (KST)**: UTC보다 9시간 빠른 **`UTC+9`**를 사용하며, 일본(JST)과 동일한 기준임
* **국가별 특성**: 영토가 넓더라도 단일 표준시를 사용하는 국가(중국)와 다중 표준시를 사용하는 국가의 차이 인지

---

### 💡 핵심 포인트 (Takeaway)
* **가설 기반 분석**: 실제 데이터를 보기 전, 비즈니스 맥락을 고려하여 결과가 어떻게 나올지 미리 **상상(Imagination)**해 보는 훈련이 중요합니다.
* **도메인 지식의 힘**: 타 직무(마케팅, 영업 등) 경험이 데이터와 결합될 때 훨씬 강력한 시너지를 낼 수 있습니다.
* **추론 능력**: 데이터가 부족하더라도 논리적 인과관계를 통해 합리적인 결론을 도출하는 역량을 키우세요.

---

### ✅ 실습 과제
**사고 실험: 무료 배송 vs 유료 배송**
* **질문**: 이커머스 환경에서 '조건 없는 무료 배송'과 '배송비 별도' 상품 중, 실제 **구매 전환율(CVR)**이 더 높게 나타나는 경우는 어느 쪽일까요?
* **생각해 볼 점**: 소비자 심리, 가격 민감도, 배송비 포함 유무에 따른 총 결제 금액의 차이를 고려하여 본인만의 논리를 세워 보세요.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Data Literacy** | 데이터를 해석하고 비판적으로 수용하여 가치를 창출하는 능력 |
| **CVR (Conversion Rate)** | 방문자 중 실제 구매나 가입 등 목표 행동을 완료한 비율 |
| **UTC (Coordinated Universal Time)** | 그리니치 천문대 기준의 국제 표준 시간 |
| **SQL Editor** | 데이터베이스에 직접 쿼리를 날려 데이터를 조작하는 도구 |
| **Why-How-What** | 문제의 본질(Why)부터 시작하는 논리적 사고 프레임워크 |


## 8교시 - 17:00 ~ 18:00

> **주제:** DBeaver 설치 및 Supabase 클라우드 데이터베이스 연동 실습
---
### 📌 오늘의 강의 핵심 요약
데이터베이스 통합 관리 도구인 **DBeaver**를 설치하고, 클라우드 환경인 **Supabase**와 로컬 PC를 연결하는 실습을 진행했습니다. 이를 통해 터미널이나 웹 GUI 없이도 데이터를 효율적으로 조회하고 관리할 수 있는 전문적인 개발 환경 구축을 완료했습니다.

---

### 📚 상세 내용 정리

#### 1️⃣ DBeaver Community Edition 설치
* **도구의 성격**: PostgreSQL, MySQL, SQLite 등 다양한 DB를 하나의 인터페이스에서 관리하는 **SQL Client**
* **설치**: 공식 홈페이지에서 무료 버전인 **Community Edition**을 OS(Windows/Mac) 환경에 맞춰 설치

#### 2️⃣ Supabase 연동 (Database Connection)
* **연결 정보 확인**: Supabase 대시보드 내 `Connect` 버튼 → `Transaction Pooler` 선택 → `Parameters` 정보 확인
* **핵심 설정값**:
    * **Host**: 데이터베이스 서버 주소
    * **Database**: 접속할 데이터베이스 이름
    * **User**: 접속 권한을 가진 사용자 ID (기본: `postgres.xxxx`)
    * **Password**: 프로젝트 생성 시 설정한 비밀번호 (분실 시 `Reset Database Password` 기능 활용)
* **DBeaver 설정**: `New Connection` → `PostgreSQL` 선택 → 위 정보 및 **Port(6543)** 입력

#### 3️⃣ 연결 결과 검증 및 데이터 조회
* **Test Connection**: 'Connected' 메시지 확인을 통해 네트워크 연결 상태 검토
* **탐색기 활용**: `Databases` → `public` → `Tables` 경로에서 `student` 테이블 확인
* **데이터 검토**: `Data` 탭을 통해 앞서 실습에서 입력했던 실제 데이터들이 정상적으로 로드되는지 최종 확인

---

### 💡 핵심 포인트 (Takeaway)
* **기초 공사의 중요성**: API 키, DB 생성, 툴 연동 등의 초기 환경 셋업은 프로젝트의 성패를 결정짓는 핵심 단계입니다.
* **도구 활용 능력**: DBeaver와 같은 클라이언트를 능숙하게 다루는 것은 데이터 직무의 기본 소양 중 하나입니다.
* **보안 관리**: 데이터베이스 인증 정보(Host, ID, PW)는 민감한 자산이므로 안전하게 보관하고 정기적으로 관리해야 합니다.

---

### ✅ 실습 과제
1. **연결 성공**: Supabase 연동 정보를 DBeaver에 정확히 입력하여 `Test Connection` 성공 메시지를 캡처합니다.
2. **테이블 조회**: DBeaver에서 `student` 테이블의 데이터 탭을 열어 정상적으로 데이터가 조회되는지 확인합니다.
3. **환경 점검**: 에디터(Antigravity), DB(Supabase), 클라이언트(DBeaver) 간의 전체적인 연동 흐름을 머릿속으로 정리해 봅니다.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **DBeaver** | 다양한 DB 브랜드와 호환되는 오픈소스 SQL 클라이언트 |
| **Connection String** | DB 접속에 필요한 정보를 담은 주소 형식의 문자열 |
| **Transaction Pooler** | 많은 수의 DB 연결을 효율적으로 중앙 관리해주는 기능 |
| **SQL Client** | DB 서버에 접속하여 쿼리를 날리고 결과를 시각적으로 보여주는 도구 |
| **BaaS (Backend as a Service)** | 백엔드 기능을 클라우드로 제공하는 서비스 (예: Supabase) |