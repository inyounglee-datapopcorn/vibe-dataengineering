# 📝 2026-01-07

---

## 1교시 09:00 - 10:00
> **주제:** 현업 데이터 엔지니어 경험담, AI 에이전트(n8n) 자동화, 데이터 파이프라인 실습

---

### 📌 오늘의 강의 핵심 요약
이 강의는 데이터 직무(엔지니어/분석가)로서의 **커리어 성장 전략**과 **AI 자동화 도구(n8n)의 활용법**을 다룹니다. 강사님은 현업에서의 고충(새벽 장애 대응 등)을 통해 **책임감(Ownership)**을 강조하며, 단순 반복 업무를 AI로 자동화하여 생산성을 높이는 방법을 시연했습니다. 또한, 서울 지하철 데이터를 활용한 **파이썬 데이터 파이프라인 구축** 및 **스케줄링 자동화**가 이번 실습의 주요 목표임을 제시합니다.

---

### 📚 상세 내용 정리

#### 1️⃣ 커리어 마인드셋 & 연봉 협상
* **시장 가치와 연봉**: 회사는 기본적으로 비용 절감을 원합니다. 따라서 본인의 연봉을 높이려면 단순 연차(경력)가 아닌, 상위 직급에 준하는 **퍼포먼스(성과)를 증명**해야 합니다.
* **주니어 vs 중니어**: 3년 차라도 5년 차 이상의 실력을 보여주는 '중니어(중고 신입+시니어)'가 되어야 하며, 이를 통해 시간을 단축하고 연봉을 높일 수 있습니다.
* **의사결정과 속도**: 변화하는 AI 시대에는 빠른 실행력과 선점이 중요합니다. 본인의 신념을 가지고 빠르게 판단하고 실행하는 '속도전'이 필수적입니다.

#### 2️⃣ AI 에이전트 & n8n 활용 (자동화 예시)
* **n8n 소개**: 5년 전 독일에서 시작된 워크플로우 자동화 도구로, 코드 없이(No-code) 또는 적은 코드로 시스템을 구축할 수 있습니다.
* **구내식당 메뉴 알림 봇 실습 예시**:
    1. **캡처(Trigger)**: 식단표 이미지를 캡처하여 업로드.
    2. **전송(Webhook)**: 이미지를 HTTP POST 방식으로 전송.
    3. **OCR(Upstage)**: 이미지 내 텍스트(메뉴명 등)를 추출.
    4. **구조화(LLM)**: 추출된 텍스트를 AI에게 지침(Prompt)을 주어 식당명, 메뉴명 등으로 깔끔하게 정리.
    5. **알림(Slack)**: 정리된 텍스트와 이미지를 슬랙 채널로 전송.
* **HR 업무 자동화 예시**: 신규 입사자 발생 시 구글 폼 제출 한 번으로 Jira 티켓 생성, Slack 계정 생성 및 초대 등을 자동화하여 반복 업무를 줄입니다.

#### 3️⃣ 데이터 엔지니어의 현실과 책임감
* **새벽 4시의 고충**: 데이터 파이프라인은 주로 새벽에 돕니다. 만약 실패하면 아침 지표가 나오지 않기 때문에, 엔지니어는 새벽에 깨어 복구해야 하는 막중한 책임을 집니다.
* **데이터의 중요성**: "물이 안 나오면 씻을 수 없듯", 데이터가 제때 공급되지 않으면 회사의 의사결정이 마비됩니다. 데이터 엔지니어는 파이프라인의 안정성을 책임지는 '토목 공사'와 같은 역할을 합니다.
* **자동화의 필요성**: 장애 탐지 및 재실행(Rerun) 같은 단순 대응을 AI 에이전트에게 맡겨, 엔지니어의 피로도를 줄이고 고부가가치 업무에 집중할 수 있게 합니다.

#### 4️⃣ 금일 및 향후 실습 계획 (Roadmap)
* **오늘의 목표**: 서울 지하철 API 데이터를 파이썬 코드로 추출하여 **DB(데이터베이스)에 적재**하고, 이를 **주기적으로 실행(스케줄링)**하도록 만드는 것까지 진행합니다.
* **단기 목표 (데이터 파이프라인)**:
    * Python & SQL 기초 활용.
    * Github Actions를 활용한 간단한 자동화.
    * Airflow 설치 및 클라우드(GCP) 가상 환경(VM) 배포.
* **최종 목표**: 코드로 100줄 이상 나오는 작업을 n8n 같은 도구로 얼마나 간단하게 구현할 수 있는지 비교 체험하고, 자신만의 자동화 프로젝트를 기획하는 것입니다.

---

### 💡 핵심 포인트 (Takeaway)
* **"비용 절감"과 "생산성"**: 회사가 AI를 도입하는 궁극적인 이유는 비용 절감입니다. 여러분은 AI를 통해 더 적은 리소스로 더 많은 일을 할 수 있음을 증명해야 합니다.
* **"신념"과 "실행"**: 남들이 하지 않을 때 먼저 배팅하고(n8n 선점 등), 확신을 가지고 밀어붙이는 태도가 커리어 성공의 열쇠입니다.
* **책임감(Ownership)**: 데이터 엔지니어링은 기술력보다 장애 상황을 끝까지 해결하려는 책임감이 더 중요한 직무일 수 있습니다.

---

### ✅ 실습/과제
1. **지하철 데이터 파이프라인 구축**: Python을 사용하여 서울 지하철 API 데이터를 가져와 DB에 넣는 코드 작성하기.
2. **자동화 비교 체험**: 이번에 짠 파이썬 로직을 나중에 n8n으로 구현하여 생산성 차이 느껴보기.
3. **프로젝트 기획**: 차주에 진행할 개인 프로젝트(어떤 데이터를 가져와서 자동화할지) 미리 구상하기.

---

### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **n8n** | 노코드/로우코드 워크플로우 자동화 툴 |
| **OCR (Optical Character Recognition)** | 광학 문자 인식 (이미지 -> 텍스트 변환) |
| **Webhook** | 특정 이벤트 발생 시 실시간으로 데이터를 전송하는 기술 |
| **Airflow** | 복잡한 데이터 파이프라인을 스케줄링하고 모니터링하는 플랫폼 |
| **Data Pipeline** | 데이터의 추출(Extract), 변환(Transform), 적재(Load) 흐름 |
| **Github Actions** | 깃허브 저장소 내에서 소프트웨어 개발 워크플로우를 자동화하는 도구 |

## 2교시 - 10:00 ~ 11:00
안녕하세요! LectureNote입니다. 📝
제공해주신 강의 녹음본을 바탕으로 수강생들이 DBeaver 활용법과 데이터 파이프라인 구축 기초를 명확히 이해할 수 있도록 정리해 드립니다.

---

### 📌 강의 핵심 요약

이번 강의에서는 **DBeaver**의 다양한 기능을 활용하여 데이터 구조(DDL)를 파악하고, 데이터 타입의 한계를 테스트(Integer Overflow)하는 실습을 진행했습니다. 또한, **서울시 지하철 실시간 위치 정보 API**를 활용한 새로운 프로젝트를 시작하며, AI에게 효과적으로 코드를 요청하기 위한 **프롬프트 엔지니어링(Few-shot 기법 등)** 전략을 학습했습니다.

---

### 📚 상세 내용 정리

#### [주제 1] DBeaver 기본 기능 및 데이터 구조 파악

* **Properties 탭 활용**: 테이블의 컬럼명, 데이터 타입, 용량(KB, MB) 등을 확인할 수 있습니다.
* **DDL (Data Definition Language)**: 데이터의 정의를 확인하는 탭입니다.
* `NOT NULL`: 비어 있으면 안 되는 값.
* `Constraint` (제약조건): PK(Primary Key) 등 데이터 무결성을 위해 건드리면 안 되는 설정 등을 확인 가능합니다.


* **코멘트(주석) 활용**: `Comment` 란에 컬럼에 대한 상세 설명(예: 거치대 수, 지역구 등)을 적어두는 습관이 중요합니다.

#### [주제 2] 데이터 타입 테스트 (오류 경험하기)

* **실습 목적**: 실무에 투입되기 전, 다양한 에러를 미리 경험해보고 데이터 타입의 한계를 이해해야 합니다.
* **Integer Overflow 테스트**:
* `int2` (SmallInt): 최대값은 **32,767**입니다.
* **테스트**: `int2` 컬럼에 40,000 같은 큰 값을 `INSERT` 시도 → **에러 발생** 확인.
* **해결**: `ALTER TABLE` 명령어로 컬럼 타입을 `int4` (Integer)로 변경 후 다시 입력 → **성공**.
* *교훈*: 그릇(데이터 타입)의 크기에 맞는 데이터를 담아야 합니다.



#### [주제 3] DBeaver 유용한 UI/UX 팁

* **Pin Tab (핀 고정)**: 탭을 우클릭하여 고정하면, 새 쿼리를 실행해도 기존 탭이 사라지지 않습니다.
* **결과창 Transpose (행/열 전환)**:
* 데이터 컬럼이 많아 가로로 보기 힘들 때 사용합니다.
* 단축키: `Tab` 키 (결과창에서) 또는 하단 버튼 클릭.
* 하나의 행(Row) 데이터를 세로로 상세하게 볼 수 있습니다.


* **스크립트(SQL Editor) 관리**:
* 새 스크립트 열기: `Ctrl + N` 또는 `SQL` 아이콘 클릭.
* 실행: `Ctrl + Enter` (Window), `Command + Enter` (Mac).
* 파일 위치 확인: 스크립트 탭 우클릭 → `Show in Explorer` (실제 파일 저장 경로 확인 가능).



#### [주제 4] 데이터 내보내기 (Advanced Copy)

* **일반 복사 문제점**: 그냥 복사하면 헤더(컬럼명)가 없고, 구분자가 탭(Tab)으로 되어 있어 활용이 불편할 수 있습니다.
* **Advanced Copy (고급 복사)**:
* 방법: 데이터 선택 → 우클릭 → `Advanced Copy` (또는 `Ctrl + Shift + C`).
* **Copy Header**: 체크 시 컬럼명 포함.
* **Delimiter (구분자)**:
* `Tap`: TSV (Tab Separated Values) 형태.
* `Comma`: CSV (Comma Separated Values) 형태. (일반적으로 많이 사용)





#### [주제 5] 새 프로젝트: 서울시 지하철 실시간 데이터

* **목표**: API를 호출하여 실시간 열차 위치 데이터를 수집하고 DB에 적재.
* **프롬프트 엔지니어링 전략 (AI 활용)**:
1. **정보 제공**: API URL, 요청 인자, 출력 값 예시를 그대로 복사해서 AI에게 제공.
2. **명확한 요구사항(Few-shot Prompting)**:
* API의 컬럼명이 직관적이지 않을 때(예: `updnline`), AI에게 "직관적으로 바꿔줘"라고만 하지 말고 **구체적인 예시**를 줍니다.
* *예시*: "`updnline`은 `up_line`처럼 Snake Case로 직관적으로 변경해줘." (하나의 예시만 줘도 AI는 패턴을 파악함)


3. **분석 목적 제시**: "원활한 지하철 운행 모니터링"과 같은 목적을 알려주면 AI가 그에 맞는 분석 리스트를 제안합니다.



---

### 💡 핵심 포인트

1. **"회사는 실수를 용납하지 않는다, 연습 때 많이 틀려보자"**: 데이터 타입을 일부러 틀리게 넣어보고 에러 메시지를 확인하는 과정이 실력 향상의 지름길입니다.
2. **도구(Tool) 숙련도**: SQL 자체도 중요하지만, DBeaver 같은 툴의 기능을 잘 다루면(단축키, 보기 모드 변경 등) 작업 효율이 훨씬 높아집니다.
3. **직관성(Intuition) 부여**: AI에게 작업을 시킬 때, 모호한 지시보다는 **구체적인 예시(Example)** 를 하나라도 주는 것이 결과물의 품질을 결정합니다.

---

### ✅ 실습/과제

1. **Integer Overflow 재현**: `int2` 타입 컬럼을 만들고 32,768 이상의 숫자를 넣어 에러 확인 후 `int4`로 변경해보기.
2. **Advanced Copy 활용**: 조회된 데이터를 헤더 포함, 콤마(,) 구분자로 복사하여 엑셀이나 메모장에 붙여넣기.
3. **지하철 데이터 프롬프트 작성**: 강의 내용을 참고하여 AI에게 데이터 수집 파이프라인 구축을 요청하는 프롬프트 작성해보기.

---

### 🔗 관련 키워드

* `DBeaver`
* `DDL (Data Definition Language)`
* `Integer Overflow`
* `CSV` / `TSV`
* `Snake Case` (명명 규칙)
* `Few-shot Prompting` (AI 프롬프팅 기법)
---

## 3교시 - 11:00 ~ 12:00
> **주제:** 지하철 데이터 파이프라인 아키텍처 설계 및 분석 가설 수립
---
### 📌 오늘의 강의 핵심 요약
본격적인 실습에 앞서 **지하철 실시간 위치 데이터 파이프라인**의 전체 구조를 설계했습니다. AI 에이전트를 활용해 프로젝트 디렉토리를 구상하고, 수집된 데이터를 통해 도출할 수 있는 비즈니스 가설(간격 정합성, 핫스팟 탐지 등)을 구체화했습니다.

---
### 📚 상세 내용 정리

#### 1️⃣ 프로젝트 디렉토리 구조 (Proposed)
* **`config.py`**: API Key, DB URL 등 환경 설정 관리.
* **`api_client.py`**: 서울시 오픈 API 호출 및 데이터 수집 인터페이스.
* **`db_client.py`**: Supabase(PostgreSQL)와의 연동 및 데이터 적재 로직.
* **`manager.py` / `main.py`**: 전체 프로세스를 총괄하고 스케줄링을 제어하는 실행 파일.

#### 2️⃣ DB 스키마 매핑 전략
* API에서 제공하는 복잡한 필드명을 직관적인 DB 컬럼명으로 변환 (예: `subwayId` -> `line_id`).
* 실시간성의 특성을 고려하여 **수집 시각(Received Time)** 기록 필수.

#### 3️⃣ 데이터 분석 목표 및 가설
* **Interval Regularity**: 열차 간격이 일정하게 유지되는지 분석하여 지연 상황 탐지.
* **Hotspot Detection**: 특정 역에서 열차 체류 시간이 평소보다 길어지는 현상 분석.
* **이상치(Outlier) 분석**: 평균 소요 시간 대비 2배 이상의 차이가 발생하는 구간 식별.

---
### 💡 핵심 포인트 (Takeaway)
* **역할의 분리**: 데이터 수집부(API)와 적재부(DB)를 코드상에서 분리하여 유지보수성을 높여야 합니다.
* **가설 중심 설계**: 무작정 데이터를 쌓는 것이 아니라, 어떤 인사이트를 얻을 것인지 먼저 정의하고 파이프라인을 설계해야 합니다.

---
### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Modularization** | 코드를 기능 단위(수집, 적재, 관리)로 나누는 작업 |
| **Schema Mapping** | 원천 데이터 원본과 대상 저장소 간의 필드 연결 |
| **Outlier Detection** | 평균 범위를 벗어나는 이상 데이터를 찾아내는 기법 |
| **Real-time Pipeline** | 실시간 발생하는 데이터를 즉시 처리하는 흐름 |

---

## 4교시 - 13:00 ~ 14:00
> **주제:** VS Code 생산성 도구 및 에이전트 표준화(`agent.md`)
---
### 📌 오늘의 강의 핵심 요약
팀원 간 개발 환경을 통일하고 AI의 결과물 파편화를 방지하기 위해 **프롬프트 표준화 파일(`agent.md`)**을 구축했습니다. 또한, 멀티 커서 등 VS Code의 고급 편집 기능을 익혀 대량의 코드를 효율적으로 수정하는 실무 팁을 학습했습니다.

---
### 📚 상세 내용 정리

#### 1️⃣ VS Code 생산성 향상 팁
* **멀티 커서(Multi-Cursor)**: 공통된 텍스트를 한 번에 수정할 때 사용. (Mac: `Cmd + D`, Win: `Ctrl + D`)
* **일괄 편집**: 여러 줄의 파일 경로 등을 드래그하여 한 번에 삭제하거나 수정함으로써 휴먼 에러를 방지하고 속도 향상.
* **마켓플레이스 활용**: 에디터 내 도구를 통해 도메인 지식(광고 모델 등) 시각화 및 개발 편의성 증대.

#### 2️⃣ 프롬프트 표준화 기법 (`agent.md`)
* **목적**: 사람마다 다른 AI 결과값(폴더 구조, 언어 등)을 방지하고 팀 프로젝트의 **환경 정합성** 유지.
* **Strict Prompting**: "Must", "Strictly"와 같은 강한 지시어를 사용하여 구조를 강제함으로써 AI가 설계도를 이탈하지 않게 설정.
* **실행 전략**: 채팅창에서 `@agent.md`를 참조시켜 표준화된 지침에 따라 프로젝트(`Seoul Subway Monitor`) 구조를 생성.

---
### 💡 핵심 포인트 (Takeaway)
* **협업의 기초는 표준화**: AI 에이전트를 활용할 때도 명확한 지침서(`agent.md`)가 있어야 팀원 모두가 동일한 코드 구조를 유지할 수 있습니다.
* **적절한 수동 개입**: AI에게 모든 것을 맡기기보다, 멀티 커서 등을 활용한 직접 편집을 병행해야 가장 효율적인 결과물이 나옵니다.

---
### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **agent.md** | AI 에이전트의 행동 지침 및 프로젝트 구조를 정의한 파일 |
| **Strict Prompting** | AI의 자유도를 제한하고 규칙을 강제하는 프롬프팅 방식 |
| **Multi-Cursor** | 동시에 여러 위치에 커서를 두어 일괄 수정하는 기능 |
| **Context Sharing** | 파일 참조를 통해 AI에게 프로젝트 배경 지식을 전달하는 것 |

---

## 5교시 - 14:00 ~ 15:00
> **주제:** Supabase 연동 실습 및 AI 모델 최적화 활용 전략
---
### 📌 오늘의 강의 핵심 요약
구축된 파이프라인에 **Supabase DB를 연동**하고, 권한 설정을 처리하는 실습을 진행했습니다. 특히 작업의 성격(설계 vs 실행)에 따라 AI 모델을 효과적으로 전환하여 효율을 극대화하는 **모델 운영 전략**을 학습했습니다.

---
### 📚 상세 내용 정리

#### 1️⃣ Supabase 보안 및 권한 설정
* **Service Role Key 활용**: 외부에서 대량의 데이터를 강제로 적재(Insert)하기 위해 `anon` 키 대신 높은 권한의 `service_role` 키를 사용.
* **보안 주의**: 높은 권한을 가진 키이므로 외부 노출에 절대 주의해야 하며, `.env` 파일을 통한 철저한 관리 필수.

#### 2️⃣ AI 모델 전환 전략 (Planning vs Execution)
* **초기 설계 (Planning/Pro)**: 복잡한 추론과 전체적인 시스템 아키텍처 구상이 필요할 때 고성능 모델 사용.
* **단순 실행 (Fast/Flash)**: 설계도가 확정된 후 단순 코드 구현이나 수정 작업 시에는 가벼운 모델로 전환하여 속도와 비용 최적화.
* **오류 방지**: 실행 단계에서 계속 Planning 모델을 쓰면 불필요한 재계획 과정으로 인해 정합성이 깨질 수 있음을 명심.

#### 3️⃣ 소스 컨트롤 및 적재 자동화
* **Git Commit**: `.env` 설정이 완료된 상태를 커밋하여 환경 보존.
* **자동 적재**: AI 에이전트가 인지한 스키마에 따라 `CREATE TABLE` 및 실시간 데이터 `INSERT` 수행.

---
### 💡 핵심 포인트 (Takeaway)
* **단계별 모델 최적화**: 계획은 똑똑한 모델에게, 단순 구현은 빠른 모델에게 맡기는 것이 프로 데이터 엔지니어의 도구 활용법입니다.
* **권한의 이해**: DB 연동 시 필요한 권한(Insert/Delete 등)에 따라 적절한 API 키를 선택해야 장애를 방지할 수 있습니다.

---
### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Service Role Key** | DB 관리 및 데이터 적재를 위한 관리자급 API 키 |
| **Planning Model** | 복잡한 추론과 계획 수립에 특화된 고성능 AI 모델 |
| **Flash/Fast Model** | 빠른 응답 속도와 효율성에 최적화된 경량 모델 |
| **Source Control** | 코드 및 환경 설정의 변경 이력을 관리하는 시스템 (Git) |

---

## 6~7교시 - 15:00 ~ 17:00
> **주제:** 파이썬 스케줄링 구현 및 데이터 파이프라인 운영 전략
---
### 📌 오늘의 강의 핵심 요약
작성된 파이프라인 코드를 **`schedule` 라이브러리**를 통해 자동화하고, 실제 운영 환경에서의 모니터링 및 트러블슈팅 방법을 익혔습니다. AI가 작성한 코드의 최종 책임은 인간에게 있다는 **Human-in-the-loop** 관점을 강조하며 실전 수집 자동화를 완성했습니다.

---
### 📚 상세 내용 정리

#### 1️⃣ 파이썬 기반 작업 자동화
* **주기적 실행**: 1분 단위로 지하철 API를 호출하여 최신 위치 데이터를 DB에 적재하는 로직 구현.
* **CLI 제어**: 터미널 환경에서 스크립트를 상시 구동하고, `Ctrl + C`를 통한 안전한 중단 방법 실습.

#### 2️⃣ 트러블슈팅 및 모니터링 (Human-in-the-loop)
* **스키마 불일치 대응**: AI가 계획과 다르게 테이블 구조를 변경했을 경우, 사용자가 DBeaver에서 직접 DDL을 확인하고 수동으로 수정하는 관리 감독 수행.
* **로그 확인**: 데이터가 정상적으로 적재되고 있는지 터미널 로그를 통해 실시간 수집 상태 모니터링.

#### 3️⃣ 데이터 엔지니어링 마인드셋
* AI가 코드를 생성해주더라도, 스키마 구조의 정합성과 데이터 품질(Quality)은 반드시 엔지니어가 직접 검토해야 함.

---
### 💡 핵심 포인트 (Takeaway)
* **최종 책임자(Reviewer)**: AI 에이전트는 효율적인 조수일 뿐, 코드의 안정성과 데이터의 정확성을 검증하는 최종 판단은 엔지니어의 몫입니다.
* **실시간성의 이해**: 스케줄링 간격을 조정하여 시스템 부하와 데이터의 최신성 사이의 균형을 맞추는 것이 핵심입니다.

---
### 🔗 관련 핵심 키워드
| 키워드 | 설명 |
| :--- | :--- |
| **Schedule** | 파이썬에서 정기적인 작업을 예약 실행하는 라이브러리 |
| **Human-in-the-loop** | AI의 작업 프로세스에 인간이 개입하여 검토하고 시정하는 방식 |
| **DDL Troubleshooting** | 테이블 구조 오류 시 SQL 명령어로 직접 구조를 수정하는 작업 |
| **Logging** | 시스템의 실행 이력과 오류를 추적하기 위한 기록 활동 |
